---
title: "Benchmarking the accelerated oblique random survival forest"
description: |
  Comparing prediction accuracy and identification of relevant predictor variables
author:
  - name: Byron C. Jaeger
affiliation: Wake Forest School of Medicine
date: "`r Sys.Date()`"
output:
  distill::distill_article:
  toc: true
toc_depth: 2
code_folding: true
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE)

source(here::here("packages.R"))

source("R/sim_surv.R")

rspec <- round_spec() |> 
  round_using_magnitude(digits = c(3, 2, 1, 1),
                        breaks = c(1, 10, 100, Inf))

names(rspec) <- paste('table.glue', names(rspec), sep = '.')

options(rspec)

tar_load(names = c(bm_pred_comb, bm_vi_comb))

```

# Benchmarking aorsf

Welcome! This benchmark experiment is under construction and will eventually be the basis of a paper that formally introduces the novel mechanisms at play in the {`aorsf`} package.  

## Methods

### Machine learning algorithms

The benchmark experiment uses real and simulated data to compare the following machine learning algorithms {`R package`}:

1. accelerated oblique random survival forests {`aorsf`}
1. original oblique random survival forests {`obliqueRSF`}
1. axis-based random survival forests {`randomForestSRC` & `ranger`}
1. axis-based conditional inference forests {`party`}
1. gradient boosted decision trees {`xgboost`}

__Note__ Presently, the original oblique random survival forest is not included in the benchmark results. This is because the original oblique random survival forest runs very slowly, turning a 10-minute benchmark into a 6 hour benchmark. I will include it once the project is more stable. 

__Note__ {`xgboost`} requires careful tuning to obtain ideal performance. While we do tune the number of boosting steps in this benchmark, we do not go beyond that and instead use default setting for hyper-parameters.

### Prediction accuracy 

The machine learning algorithms above are compared based on their discrimination (C-statistic) and scaled Brier score, which are computed using the `riskRegression` package, and specifically `riskRegression::Score()`. Computational time required for model fitting and model predictions are also presented. Results are averaged over 15 runs of Monte-Carlo Cross-validation. In each run of Monte-Carlo Cross-validation, the current data are separated into a training and testing set, all modeling techniques are applied to the training set, and model predictions are evaluated in the testing set. 

### Variable importance 

I compare four different techniques to measure variable importance (VI).

1. `Negation importance`: the method I propose to develop further with this pilot application. This is computed using an accelerated oblique random survival forest model.

1. `ANOVA importance`: A method introduced by Menze et al. This is computed using an accelerated oblique random survival forest model.

1. `SHAP importance`: A method introduced by Lundberg et al. SHAP importance is widely regarded as a gold standard due to its excellent asymptotic properties. This is computed using a boosting model.

1. `Permutation importance`: A method introduced by Leo Breiman. This is computed using a standard random survival forest model.

__Evaluation of variable importance techniques__: VI techniques are compared based on their discrimination (C-statistic) between relevant and irrelevant predictor variables. The relevant predictor variables are those that have a relationship to the outcome, and the irrelevant variables are those with no relationship to the outcome. This is equivalent to assigning a binary outcome to each variable: 1 if it is relevant, 0 otherwise, and then computing a C-statistic for discrimination of the relevant versus irrelevant variables using VI as a 'prediction' for the variables.

__Details on C-statistic computation:__ The C-statistic for VI is not the same thing as the C-statistic for prediction accuracy. Since relevance of a variable is binary, the C-statistic for VI is computed using `yardstick::roc_auc`, which wraps `pROC::roc`. 

## Data sets

### Mayo Clinic Primary Biliary Cholangitis Data, N = 276

Primary sclerosing cholangitis is an autoimmune disease leading to destruction of the small bile ducts in the liver. Progression is slow but inexhortable, eventually leading to cirrhosis and liver decompensation. The condition has been recognised since at least 1851 and was named "primary biliary cirrhosis" in 1949. Because cirrhosis is a feature only of advanced disease, a change of its name to "primary biliary cholangitis" was proposed by patient advocacy groups in 2014.

This data is from the Mayo Clinic trial in PBC conducted between 1974 and 1984. A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval, met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine. The first 312 cases in the data set participated in the randomized trial and contain largely complete data. The additional 112 cases did not participate in the clinical trial, but consented to have basic measurements recorded and to be followed for survival. Six of those cases were lost to follow-up shortly after diagnosis, so the data here are on an additional 106 cases as well as the 312 randomized participants. The `pbc_orsf` data includes 276 participants who had no missing data for the study covariates. The first 10 rows of the data are printed here:

```{r}
as_tibble(pbc_orsf)
```

### Rotterdam Breast Cancer Data, N = 2,982

The `rotterdam` data set includes 2982 primary breast cancers patients whose records were included in the Rotterdam tumor bank. These data sets are used in a paper by Royston and Altman that is referenced below. The first 10 rows of the data are printed here:

```{r}
as_tibble(survival::rotterdam)
```

### Simulated data

The simulated data contain right-censored time-to-event outcomes with five types of predictor variables:

- $z$ (irrelevant) variables $z_1$, $\dots$, $z_{k}$ had no relationship with the outcome.

- $x$ (linear effect) variables $x_1$, $\dots$, $x_{k}$ had a linear relationship with the outcome.

- $w$ (non-linear effect) variables $w_1$, $\dots$, $w_{k}$ had a non-linear relationship with the outcome. The non-linear relationship followed a sinusoidal curve with varying period length.

- $g$ (conditional linear) variables $g_1$, $\dots$, $g_{k}$ had a conditional linear relationship with the outcome. In other words, for each $i \in \{1, \ldots, k\}$,  $g_i$ had no relationship to the outcome, but $g_i * x_i$ was linearly related to the outcome.

- $v$ (combination linear) variables $v_1$, $\dots$, $v_{k}$ had a combination linear relationship with the outcome. In other words, $v_1, v_2, v_3$ each had no effect on the outcome individually, but the latent variable $v_1*c_1* + v_2*c_2 + v_3 * c_3$ was linearly related to the outcome. Coefficients $c_i$ were generated at random uniformly from -1 to -0.5 and 0.5 to 1.

__Mean and standard deviation__ The multivariate normal distribution that generated values for the predictor matrix had a mean of zero and standard deviation of 1 for each variable.

__Correlation__: The correlation among $z$ variables was allowed to be non-zero as was the correlation among $x$ variables and the correlation between $x$ and $z$ variables. For all non-zero correlations, a value was selected at random uniformly between a lower and upper boundary. We set the lower boundary as -0.1 and upper boundary as +0.1. The correlations were adjusted if needed to create the nearest positive definite covariance matrix using `Matrix::nearPD()`.

__Effect sizes__: A total effect of 6 was distributed evenly across each predictor group. For example, with a total of $k$ linear effect predictors (i.e., the $x$ variables), the increase in log-hazard associated with a 1 unit increased in each $x$ variable was $6 / 25 = 0.24$, i.e., about a 27% increase in relative risk.

__Outcomes__: A censored outcome variable was generated with the `simsurv::simsurv()` function using a predictor matrix that included all relevant predictors. After the outcome was generated the interactions, non-linear transformations, and linear combinations of relevant predictors were dropped from the data.

#### Demo and validation of simulated data

Some tests of the simulated data are presented here. If you have no skepticism regarding the simulated data, you can safely skip this sub-section. For simplicity, we set the number of predictors ($k$) to 3 for each predictor group.

__Data generation__: There are no tests for this step, just a glimpse of the simulated set of data.

```{r}

set.seed(32987)

ss_demo <-
  sim_surv(
    n_obs = 10000,
    n_z = 3,
    n_x = 3,
    n_g = 3,
    n_w = 3,
    n_v = 3,
    # omit categorical variables; these are under construction
    n_c = 0
  )

glimpse(ss_demo$data)

```

__Column means__: This test verifies that the mean of each predictor is close to zero, and that roughly 45% of outcomes were censored.

```{r}

data_sim <- ss_demo$data

data_sim$true_v <-
  with(
    data_sim,
      v1 * ss_demo$coefs_lc[[1]][1] +
      v2 * ss_demo$coefs_lc[[1]][2] +
      v3 * ss_demo$coefs_lc[[1]][3]
  )

data_means <- data_sim |>
  select(where(is.numeric)) |>
  apply(2, mean) |>
  round(3) |>
  enframe()

# we can see that the mean of each predictor is close to zero, and that roughly 40% of outcomes were censored.
data_means

```

__Effects detected__: This test verifies that, when the oracle model is used, an effect is only detected for variables that have a relationship to the outcome.

```{r}

dd <- datadist(data_sim)
options(datadist = dd)

mdl <- cph(
  Surv(time, status) ~
    # z variables; no effect should be detected
    z1 + z2 + z3 +
    # x variables; effect should be detected
    # g variables; no effect should be detected
    # interaction between x and g; effect should be detected
    x1*g1 + x2*g2 + x3*g3 +
    # w variables; effect should be detected and should be nonlinear
    rcs(w1) + rcs(w2) + rcs(w3) +
    # v variables; no effect should be detected for individual
    # variables v1, v2, and v3, but the linear combination of
    # these variables should have a detectable effect.
    # note: v1 is not included due to co-linearity
    v2 + v3 + true_v,
  data = data_sim
)

# All expectations outlined above are met!
anova(mdl)

```

__Effect size estimation for linear effects__: This test verifies that, when the oracle model is used, the estimated effect sizes for variables with linear effects are consistent with the effect sizes used to simulate the data.

```{r}

effects_expected <- c("x1" = 2,
                      "x2" = 2,
                      "x3" = 2,
                      "g1" = 0,
                      "g2" = 0,
                      "g3" = 0,
                      "x1 * g1" = 2,
                      "x2 * g2" = 2,
                      "x3 * g3" = 2,
                      "v2" = 0,
                      "v3" = 0,
                      "true_v" = 2)

effects_observed <- coef(mdl)[names(effects_expected)]

# the differences between observed and expected estimates is small
abs(effects_expected - effects_observed) |>
  enframe(name = 'predictor',
          value = 'bias')

```

__Effect size estimation for non-linear effects__: This test verifies that, when the oracle model is used, the estimated effect sizes for variables with non-linear effects are consistent with the effect sizes used to simulate the data.

```{r}

data_gg <-
  list(w1 = as.data.frame(rms::Predict(mdl, w1)) |> select(x=w1, yhat),
       w2 = as.data.frame(rms::Predict(mdl, w2)) |> select(x=w2, yhat),
       w3 = as.data.frame(rms::Predict(mdl, w3)) |> select(x=w3, yhat)) |>
  bind_rows(.id = 'xvar') |>
  filter(x > -2.5, x < 2.5) |>
  mutate(xvar = recode(xvar,
                       w1 = "Non-linear variable: w1",
                       w2 = "Non-linear variable: w2",
                       w3 = "Non-linear variable: w3")) |>
  as_tibble()

# We expect increasing wiggle from left to right.

ggplot(data_gg) +
  aes(x = x, y = yhat) +
  geom_line() +
  facet_wrap(~ xvar) +
  theme_bw() +
  labs(x = 'W value', y = 'Estimated log-hazard')

```

These results verify that the simulated data set has the expected properties


## Results

### Prediction accuracy

Overall, {`aorsf`} had the highest prognostic accuracy (__Table 1__), obtaining the highest C-statistic and scaled brier score in each of the three datasets. In addition, {`aorsf`} grew tree ensembles about 10 times as fast as {`ranger`} and generated predictions about 25 times as fast as {`party`}. Not bad! While {`aorsf`} was slower than {`randomForestSRC`}, its predictions were substantially more accurate.  

__Table 1__ Prognostic accuracy and computational efficiency of machine learning algorithms applied to real and simulated data.

```{r}

results_overall <- bm_pred_comb |> 
  mutate(model = recode(model, 'cif' = 'party')) |> 
  group_by(model) |>
  summarize(
    across(
      .cols = c(cstat, IPA, time_fit, time_prd),
      .fns = list(mean = mean, sd = sd)
    )
  ) |> 
  mutate(data = 'Overall')

data_tbl <- bm_pred_comb |> 
  mutate(model = recode(model, 'cif' = 'party')) |> 
  group_by(model, data) |>
  summarize(
    across(
      .cols = c(cstat, IPA, time_fit, time_prd),
      .fns = list(mean = mean, sd = sd)
    )
  ) |> 
  bind_rows(results_overall) |> 
  group_by(data) |> 
  mutate(
    across(.cols = starts_with("time"), .fns = as.numeric),
    time_fit_ratio = time_fit_mean / time_fit_mean[model == 'aorsf'],
    time_prd_ratio = time_prd_mean / time_prd_mean[model == 'aorsf']
  ) |> 
  transmute(
    model, 
    data = recode(
      data,
      'pbc_orsf' = 'Mayo Clinic Primary Biliary Cholangitis Data, N = 276',
      'rotterdam' = 'Rotterdam Breast Cancer Data, N = 2,982',
      'sim' = 'Simulated data'
    ),
    cstat = table_glue("{cstat_mean} ({cstat_sd})"),
    ipa = table_glue("{IPA_mean} ({IPA_sd})"),
    time_fit_mean = format(round(time_fit_mean, 4), nsmall = 4),
    time_fit_ratio = table_value(time_fit_ratio),
    time_prd_mean = format(round(time_prd_mean, 4), nsmall = 4),
    time_prd_ratio = table_value(time_prd_ratio)
  )

gt(data_tbl, rowname_col = 'model', groupname_col = 'data') |> 
  cols_label(cstat = 'C-Statistic (SD)',
             ipa = 'Scaled Brier Score (SD)',
             time_fit_mean = 'Mean, seconds',
             time_fit_ratio = 'Ratio',
             time_prd_mean = 'Mean, seconds',
             time_prd_ratio = 'Ratio') |> 
  cols_align(align = 'center') |> 
  tab_spanner(label = 'Model prediction accuracy',
              columns = c('cstat', 'ipa')) |> 
  tab_spanner(label = 'Model fit time',
              columns = c('time_fit_mean', 'time_fit_ratio')) |> 
  tab_spanner(label = 'Risk prediction time',
              columns = c('time_prd_mean', 'time_prd_ratio')) 


```


### Variable importance

__Table 2__: Discrimination between relevant and irrelevant predictor variables for each VI technique in . Cells that are highlighted with a light blue color indicate which variable importance technique had the best discrimination for a given sample size, correlation setting, and variable type. 

```{r}

data_gt <- bm_vi_comb |>
  group_by(n_obs, xcorr, model) |> 
  summarize(
    across(
      .cols = c(overall, x, w, g, v),
      .fns = function(x) table_glue("{mean(x)} ({sd(x)})")
    )
  ) |> 
  ungroup() |> 
  mutate(
    xcorr = recode(
      xcorr, 
      '0.0' = '|Corr(linear effect variables, irrelevant variables)| = 0',
      '0.1' = '|Corr(linear effect variables, irrelevant variables)| \u2264 0.10',
      '0.2' = '|Corr(linear effect variables, irrelevant variables)| \u2264 0.20',
      '0.3' = '|Corr(linear effect variables, irrelevant variables)| \u2264 0.30',
      '0.4' = '|Corr(linear effect variables, irrelevant variables)| \u2264 0.40'),
    model = factor(
      model,
      levels = c('aorsf',
                 'aorsf_menze',
                 'xgboost',
                 'randomForestSRC',
                 'ranger'),
      labels = c('Negation importance',
                 'ANOVA importance',
                 'SHAP importance',
                 'Permutation importance (SRC)',
                 'Permutation importance (ranger)')
    )
  ) |> 
  select(model, overall, x, w, g, v) |> 
  arrange(model)


tbl <- data_gt |> 
  gt(rowname_col = 'model', groupname_col = 'xcorr') |> 
  tab_stubhead(label = 'Variable importance technique') |> 
  cols_label(
    overall = "All effects",
    x = "Linear effects",
    w = "Non-linear effects",
    g = "Interaction effects",
    v = "Combo effects") |> 
  cols_align(align = 'center')

tbl

```


## Conclusion

{`aorsf`} offers novel, computationally efficient techniques to fit and interpret oblique random survival forests. The benchmarks above are incomplete, and I will be developing them more over time as I slowly work towards a paper. 

# REFERENCES

1. Source for `pbc_orsf` data: T Therneau and P Grambsch (2000), Modeling Survival Data: Extending the Cox Model, Springer-Verlag, New York. ISBN: 0-387-98784-3.

1. Source for `rotterdam` data: Patrick Royston and Douglas Altman, External validation of a Cox prognostic model: principles and methods. BMC Medical Research Methodology 2013, 13:33
