\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL

\usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

% extra packages
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{multirow}
\hypersetup{hidelinks}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{bm}
\usepackage{algorithm, algcompatible, algpseudocode}
\usepackage{eqparbox}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newdimen{\algindent}
\setlength\algindent{1.5em}          % algorithmic indent=1.5em
\algnewcommand\LeftComment[2]{%
\hspace{#1\algindent}$\triangleright$ \eqparbox{COMMENT}{#2} \hfill %
}

\algnewcommand{\algorithmicgoto}{\textbf{go to}}%
\algnewcommand{\Goto}[1]{\algorithmicgoto~\ref{#1}}%

\DeclareCaptionLabelFormat{AppendixTables}{A.#2}

\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\cstat}{\widehat{\text{C}}(t)}
\newcommand{\bstat}{\widehat{\text{BS}}(t)}
\newcommand{\bsbar}{\mathcal{\widehat{BS}}(t_1, t_2)}
\newcommand{\bskap}{\mathcal{\widehat{BS}}_0(t_1, t_2)}

\newcommand{\ntrain}{N_{\text{train}}}
\newcommand{\ntest}{N_{\text{test}}}

\newcommand{\secref}[1]{Section \ref{#1}}

\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\tabrefAppendix}[1]{Table A.\ref{#1}}

\newcommand{\nope}[1]{}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

<<echo = FALSE, include = FALSE>>=

setwd(here::here())
source("packages.R")
library(knitr)
library(kableExtra)

knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      dpi = 600,
                      cache = FALSE,
                      warning = FALSE)

rspec <- round_spec() %>%
  round_using_magnitude(digits = c(3, 2, 1, 1),
                        breaks = c(1, 10, 100, Inf))

names(rspec) <- paste('table.glue', names(rspec), sep = '.')

options(rspec)

tar_load(names = c(bm_pred_clean,
                   bm_vi_viz,
                   bm_pred_viz,
                   bm_pred_model_viz,
                   bm_pred_time_viz,
                   bm_time_viz,
                   clincalc_r2,
                   data_key,
                   model_key))

bm_pred_n_runs <- max(bm_pred_clean$data$run)

bm_pred_total_time <-
  sum(with(bm_pred_clean$data, time_fit + time_pred)) / (60^2)

n_learners <- nrow(model_key)

n_data_sets <- length(unique(data_key$label))
n_risk_tasks <- nrow(data_key)

n_obs_model <- table_value(
  as.integer(n_risk_tasks*bm_pred_n_runs*n_learners)
)

nrow_median <- table_value(median(data_key$nrow))
nrow_max <- table_value(max(data_key$nrow))
nrow_min <- table_value(min(data_key$nrow))

ncol_median <- table_value(median(data_key$ncol))
ncol_max <- table_value(max(data_key$ncol))
ncol_min <- table_value(min(data_key$ncol))

pcens_median <- table_value(100*median(data_key$pcens))
pcens_max    <- table_value(100*max(data_key$pcens))
pcens_min    <- table_value(100*min(data_key$pcens))

@

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Accelerated and interpretable oblique random survival forests}
  \author{Byron C.~Jaeger\thanks{
    The authors gratefully acknowledge \textit{the Center for Biomedical Informatics, Wake Forest University School of Medicine for supporting this research. The project was also supported by the National Center for Advancing Translational Sciences (NCATS), National Institutes of Health through Grant Award Number UL1TR001420. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. Dr. Pajewski was supported by grant number P30AG021332 from the National Institutes of Health, while Dr. Speiser was supported by K25AG068253.}}\hspace{.2cm}\\
   Dept. of Biostatistics and Data Science,
	 Wake Forest Univ. School of Medicine\\
  and \\
   Sawyer Welden \\
   Dept. of Biostatistics and Data Science,
   Wake Forest Univ. School of Medicine \\
  and \\
   Kristin Lenoir \\
   Dept. of Biostatistics and Data Science,
   Wake Forest Univ. School of Medicine\\
  and \\
   Jaime L.~Speiser \\
	 Dept. of Biostatistics and Data Science,
	 Wake Forest Univ. School of Medicine\\
	and \\
	 Matthew W. Segar \\
   Dept. of Cardiology,
   Texas Heart Institute \\
  and \\
   Ambarish Pandey \\
   Division of Cardiology, Dept. of Internal Medicine, \\
   University of Texas Southwestern Medical Center \\
  and \\
   Nicholas M.~Pajewski \\
	 Dept. of Biostatistics and Data Science,
	 Wake Forest Univ. School of Medicine
  }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Accelerated and interpretable oblique random survival forests}
\end{center}
  \medskip
} \fi

\newpage
\begin{abstract}
	The oblique random survival forest (RSF) is an ensemble supervised learning method for right-censored outcomes. Trees in the oblique RSF are grown using linear combinations of predictors, whereas in the standard RSF, a single predictor is used. Oblique RSF ensembles have high prediction accuracy, but assessing many linear combinations of predictors induces high computational overhead. In addition, few methods have been developed for estimation of variable importance (VI) with oblique RSFs. We introduce a method to increase computational efficiency of the oblique RSF and a method to estimate VI with the oblique RSF. Our computational approach uses Newton-Raphson scoring in each non-leaf node, We estimate VI by negating each coefficient used for a given predictor in linear combinations, and then computing the reduction in out-of-bag accuracy. In benchmarking experiments, we find our implementation of the oblique RSF is hundreds of times faster, with equivalent prediction accuracy, compared to existing software for oblique RSFs. We find in simulation studies that `negation VI' discriminates between relevant and irrelevant numeric predictors more accurately than permutation VI, Shapley VI, and a technique to measure VI using analysis of variance. All  oblique RSF methods in the current study are available in the \texttt{aorsf} R package, and additional supplemental materials are available online.
\end{abstract}

\noindent%
{\it Keywords:}  Supervised learning, Computational efficiency, Variable importance
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!

\section{Introduction}

Risk prediction may reduce the burden of disease by guiding strategies for prevention and treatment in a wide range of domains \citep{moons2012riskI}. The random survival forest (RSF; \citet{ishwaran2008random, hothorn2006unbiased}) is a supervised learning algorithm that has been used frequently for risk prediction \citep{wang2017selective}. Similar to random forests (RFs) for classification and regression \citep{breiman2001random}, The RSF is a large set of de-correlated and randomized decision trees, with each tree contributing to the ensemble's prediction function. Notable characteristics of the RSF include uniform convergence of its ensemble survival prediction function to the true survival function, first shown by \citet{ishwaran2010consistency} and later by \citet{cui2017consistency} under more general conditions. However, \citet{cui2017consistency} noted that the RSF is at a disadvantage when predictors are correlated and some are not relevant to the censored outcome, which is a strong possibility when large clinical and `omic' databases are leveraged for risk prediction.

A potential approach to improve the RSF when predictors are correlated and some are not relevant to the censored outcome is to use oblique trees instead of axis based trees. Axis based trees split data using a single predictor, creating decision boundaries that are perpendicular or parallel to axes of the predictor space \citep[see][Chapter~2]{breiman2017classification}. Oblique trees split data using a linear combination of predictors, creating decision boundaries that are neither parallel nor perpendicular to axes of their contributing predictors \citep[see][Chapter~5]{breiman2017classification}. Oblique trees may create more adequate partitions of a predictor space compared to axis-based trees, as demonstrated in Figure \ref{fig:axis_v_oblique}. \citet{menze2011oblique} examined prediction accuracy of RFs in the presence of correlated predictors and found that oblique RFs had substantially higher prediction accuracy compared to axis-based RFs. Similarly, \citet{jaeger2019oblique} found that growing RSFs with oblique rather than axis-based trees reduced the RSF's concordance error, with improvements ranging from 2.5\% to 24.9\% depending on the data analyzed.

<<axis_v_oblique, fig.height=5, fig.width=12, fig.cap="Decision boundaries from an axis based (panel A) and oblique (panel B) decision tree used to classify penguin species based on bill depth and bill length. The decision boundary from the oblique tree is better able to capture the geometry of this data, leading to fewer mis-classified penguins.">>=

cols <- c("darkorange", "purple", "cyan4")

penguins <- drop_na(palmerpenguins::penguins)

simple_scale <- function(x) (x - mean(x))/sd(x)

penguins_scaled <- penguins %>%
  mutate(x = simple_scale(bill_length_mm),
         y = simple_scale(bill_depth_mm)) %>%
  filter(x > -3, x < 3, y > -2, y < 2)

poly1_axis <- data.frame(
 x = c(-3, 0.2116068, 0.2116068, -3, -3),
 y = c(-2, -2,   2, 2, -2)
)

poly2_axis <- data.frame(
 x = c(0.2116068, 3, 3, 0.2116068, 0.2116068),
 y = c(-0.4137976, -0.4137976, -2, -2, -0.4137976)
)

poly3_axis <- data.frame(
 x = c(0.2116068, 3, 3, 0.2116068, 0.2116068),
 y = c(2, 2, -0.4137976, -0.4137976, 2)
)

poly1 <- data.frame(
  x = c(-3, -1.8, .8, -3, -3),
  y = c(-2, -2,   2, 2, -2)
)

poly2 <- data.frame(
  x = c(-1.35, .8,  3,  3, -1.35),
  y = c(-1.3,   2,  2, .9, -1.3)
)

poly3 <- data.frame(
  x = c(-1.8, -1.35, 3, 3, -1.8),
  y = c(-2,    -1.3, .9, -2, -2)
)

p1 <- ggplot(data = penguins_scaled) +
  aes(x = x, y = y) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_polygon(data=poly1_axis, fill=cols[1], color='black', alpha=0.2) +
  geom_polygon(data=poly2_axis, fill=cols[2], color='black', alpha=0.2) +
  geom_polygon(data=poly3_axis, fill=cols[3], color='black', alpha=0.2) +
  theme_minimal() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  labs(y = "Bill depth, scaled",
       x = "Bill length, scaled") +
  coord_cartesian(ylim = c(-2.1, 2.1),
                  xlim = c(-3.1, 3.1)) +
  theme(panel.grid = element_blank(),
        legend.position = '',
        text = element_text(size = 12))

p2 <- ggplot(data = penguins_scaled) +
  aes(x = x, y = y) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_polygon(data=poly1, fill=cols[1], color='black', alpha=0.2) +
  geom_polygon(data=poly2, fill=cols[2], color='black', alpha=0.2) +
  geom_polygon(data=poly3, fill=cols[3], color='black', alpha=0.2) +
  theme_minimal() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  labs(y = "",
       x = "Bill length, scaled") +
  coord_cartesian(ylim = c(-2.1, 2.1),
                  xlim = c(-3.1, 3.1)) +
  theme(panel.grid = element_blank(),
        legend.position = '',
        text = element_text(size = 12))

ggarrange(tag_facet(p1, tag_pool = "A", vjust = 2.5),
          tag_facet(p2, tag_pool = "B", vjust = 2.5),
          nrow = 1)

@


Conceptually, oblique trees are similar to methods that use covariate rotation or extension to generate linear combinations of predictors prior to growing axis-based trees \citep{zhou2016random, wang2017random}. The difference is that oblique trees generate linear combinations of predictors within each non-leaf node, using only the data and predictors associated with that node, instead of generating the linear combinations prior to growing the tree. This ``node-specific'' approach to creating linear combinations of predictors leads to greater diversity in oblique tree ensembles, which may lead to greater prediction accuracy \citep{breiman2001random}.

Despite the potential for higher accuracy, oblique trees have at least two notable drawbacks compared to axis-based trees. First, finding a locally optimal oblique decision rule may require exponentially more computation than an axis-based rule. If $p$ predictors are potentially used to split $n$ observations, up to $\mathcal{O}(n^p)$ oblique splits can be assessed versus $\mathcal{O}(n \cdot p)$ axis-based splits \citep{heath1993induction, murthy1994system}. Second, although variable importance (VI) is one of the most widely used strategies to interpret RFs \citep{ishwaran2019standard}, few studies have investigated VI for oblique RFs \citep[see][Section~5]{menze2011oblique}, and fewer have investigated VI specifically for the oblique RSF. Without general methodology to estimate VI, interpretation of oblique RFs is challenging.

The aim of this paper is to introduce methodology that improves the computational efficiency and interpretation of oblique RSFs. \secref{sec:methods} reviews prior work, introduces our method to reduce the computational cost of oblique RSFs (\ie, accelerate them), and introduces `negation VI', a method to estimate VI with oblique RSFs that does not require permutation of data. We describe benchmarking experiments and simulation studies to evaluate these methods in \secref{sec:numeric}, and present results in \secref{sec:results}. In \secref{sec:discussion}, we summarize results from the current study, connecting our findings to prior work and outlining potential future research topics. All oblique RSF methods introduced in the current study are available in the \texttt{aorsf} R package \citep{jaeger2022aorsf}.

\section{Methods and materials} \label{sec:methods}

Sections \ref{sec:rw_forests} and \ref{sec:rw_vi} briefly summarize prior studies that have developed methods related to the oblique RSF and VI, respectively. \secref{sec:aorsf} describes our approach to reduce computational overhead of the oblique RSF and \secref{sec:negation_vi} introduces negation VI, a novel technique to estimate VI in oblique RFs.

\subsection{Axis-based and oblique random forests} \label{sec:rw_forests}

After \citet{breiman2001random} introduced the axis-based and oblique RF, numerous methods were developed to grow oblique RFs for classification or regression tasks \citep{menze2011oblique, zhang2014oblique, rainforth2015canonical, zhu2015reinforcement, poona2016investigating, qiu2017oblique, tomita2020sparse, katuwal2020heterogeneous}. However, oblique splitting approaches for classification or regression may not generalize to censored outcomes \citep[\eg, see][Section~4.5.1]{zhu2013tree}, and most research involving the RSF has focused on forests with axis-based trees \citep{wang2017selective}.

\citet{hothorn2006unbiased} developed an axis-based RSF in their framework for unbiased recursive partitioning, more commonly referred to as the conditional inference forest (CIF). \citet{zhou2016random} developed a rotation forest based on the CIF and \citet{wang2017random} developed a method for extending the predictor space of the CIF. \citet{ishwaran2008random} developed an axis-based RSF with strict adherence to the rules for growing trees proposed in \citet{breiman2001random}.  \citet{jaeger2019oblique} developed the oblique RSF following the bootstrapping approach described in Breiman's original RF and incorporating early stopping rules from the CIF.

Fast algorithms to fit axis-based RSFs are available in the \texttt{randomForestSRC} R package \citep{randomForestSRC} and the \texttt{ranger} \citep{ranger} R package. \texttt{randomForestSRC} provides a unified interface to grow RFs in a wide range of analyses, and \texttt{ranger} is designed to grow RFs efficiently using high dimensional data. Fast algorithms to fit the CIF are provided by the \texttt{party} R package \citep{hothorn2010party}, which provides a computational toolbox for recursive partitioning using conditional inference trees. \citet{jaeger2019oblique} developed the \texttt{obliqueRSF} package and found it was approximately 30 times slower than \texttt{party} and nearly 200 times slower than \texttt{randomForestSRC}. Few studies have developed software with fast algorithms for oblique RSFs that have comparable speed compared to algorithms for axis-based RSFs.

\subsection{Variable importance} \label{sec:rw_vi}

Several techniques to estimate VI have been developed since \citet{breiman2001random} introduced permutation VI, which is defined for each predictor as the difference in a RF's estimated prediction error before versus after the predictor's values are randomly permuted. \citet{strobl2007bias} identified bias in permutation VI driven by categorical predictors and bootstrap sampling, and proposed a permutation VI measure based on unbiased recursive partitioning \citep{hothorn2006unbiased}. \citet{menze2011oblique} introduced an approach to estimate VI for oblique RFs that computes an analysis of variance (ANOVA) table in non-leaf nodes to obtain p-values for each predictor contributing to the node. The ANOVA VI\footnote{\citet{menze2011oblique} name their method `oblique RF VI', but we use the name `ANOVA VI' in this article to avoid confusing Menze's approach with other approaches to estimate VI for oblique RFs.} is then defined for each predictor as the number of times a p-value associated with the predictor is $\leq 0.01$ while growing a forest. \citet{lundberg2017unified} introduced a method to estimate VI using SHapley Additive exPlanation (SHAP) values, which estimates the contribution of a predictor to a model's prediction for a given observation. SHAP VI is computed for each predictor by taking the mean absolute value of SHAP values for that predictor across all observations in a given set. With the exception of \citet{menze2011oblique}, few studies have evaluated estimation of VI using oblique RFs, and fewer have examined VI specifically for the oblique RSF.

% Several supervised learning algorithms can develop prediction functions for right-censored time-to-event outcomes, henceforth referred to as survival outcomes. \cite{ishwaran2008random} developed the RF for survival, an extension of the RF for regression and classification developed by \citet{breiman2001random}.

\subsection{The accelerated oblique random survival forest} \label{sec:aorsf}

Consider the usual framework for right-censored time-to-event outcomes with training data $$\dataset_{\text{train}} = \left\{ (T_i, \delta_i, \bm{x}_{i}) \right\}_{i=1}^{N_{\text{train}}}.$$ Here, $T_i$ is the event time if $\delta_i=1$ or the censoring time if $\delta_i=0$, and $\bm{x}_i$ is a vector of predictors values. Assuming there are no ties, let $t_1 < \, \ldots \, < t_m$ denote the $m$ unique event times in $\dataset_{\text{train}}$.

To accelerate the oblique RSF, we propose to identify linear combinations of predictor variables in non-leaf nodes by applying Newton Raphson scoring to the partial likelihood function of the Cox regression model:
\begin{equation}\label{eqn:cox-partial-likelihood}
L(\bm\beta) = \prod_{i=1}^m \frac{e^{\bm{x}_{j(i)}^T \bm\beta}}{\sum_{j \in R_i} e^{\bm{x}_j^T \bm\beta}},
\end{equation}
where $R_i$ is the set of indices, $j$, with $T_j \geq t_i$ (\ie, those still at risk at time $t_i$), and $j(i)$ is the index of the observation for which an event occurred at time $t_i$. Newton Raphson scoring is an exceptionally fast estimation procedure, and the \texttt{survival} package \citep{survival} includes documentation that outlines how to efficiently program it \citep{therneau_survival_2022}. As described in \cite{therneau2000cox}, a vector of estimated regression coefficients, $\hat{\bm{\beta}}$, is updated in each step of the procedure: $$\hat{\bm{\beta}}^{k+1} =  \hat{\bm{\beta}}^{k} + U(\hat{\bm{\beta}} = \hat{\bm{\beta}}^{k})\, \mathcal{I}^{-1}(\hat{\bm{\beta}} = \hat{\bm{\beta}}^{k}),$$ where $U(\hat{\bm{\beta}})$ is the score vector and $\mathcal{I}^{-1}(\hat{\bm{\beta}})$ is the inverse of the observed information matrix. After obtaining $\hat{\bm\beta}$, a linear combination of variables, $\eta$, is obtained by computing $\eta = \bm{x}^T \hat{\bm{\beta}}$.


For statistical inference, it is recommended to continue updating $\hat{\bm{\beta}}$ by completing additional iterations of Newton Raphson scoring until a convergence threshold is met. However, since an estimate of $\hat{\bm{\beta}}$ is created by the first iteration of Newton Raphson scoring, only one iteration of Newton Raphson scoring is needed to identify a valid linear combination of predictors. In \secref{sec:results_pred}, we formally test whether growing oblique survival trees using one iteration of Newton Raphson scoring provides equivalent prediction accuracy compared to trees where iterations are completed until a convergence threshold is met.

Algorithm \ref{alg:aorsf} presents our approach to fitting an oblique survival tree in the accelerated oblique RSF using default values from the \texttt{aorsf} R package. Several steps are taken to reduce computational overhead. First, memory is conserved by conducting bootstrap resampling via random integer-valued weights, rather than using a bootstrapped copy of the original data. Memory conservation also takes place in terminal nodes, where we restrict estimation of the survival and cumulative hazard function to event times that occur among observations in the node. Second, early stopping is applied to the tree-growing procedure if a statistical criterion is not met. In our case, the criterion is based on the magnitude of a log-rank test statistic corresponding to splitting the data at a current node. Third, instead of greedy recursive partitioning, we use `good enough' partitioning. More specifically, instead of computing a log-rank test statistic for several different linear combinations of variables and proceeding with the highest scoring option, we identify an optimal cut-point for one linear combination of variables and assess whether using this combination will create a split that passes the criterion for splitting a node. If it does not pass the criterion, then another linear combination will be tested, with the maximum number of attempts set by the parameter \texttt{n\_retry}. Often a `good-enough` split can be found in just one attempt when the training set is large, which gives the accelerated oblique RSF a computational advantage in larger training sets compared to greedy partitioning.

\spacingset{1}
\begin{algorithm}
    \caption{Accelerated oblique random survival tree using default parameters.} \label{alg:aorsf}
  \begin{algorithmic}[1]
    \Require Training data $\dataset_{\text{train}} = \left\{ (T_i, \delta_i, \bm{x}_{i}) \right\}_{i=1}^{N_{\text{train}}}$, $\text{mtry} = \sqrt{\text{ncol}(\bm{x}_{\text{train}})}$, $\text{n\_split} = 5$, $\text{n\_retry} = 3$, and $\text{split\_min\_stat} = 3.841459$
    \State $\mathcal{T} \gets \emptyset$
    \State $w \gets \text{sample}(\text{from} = \left\{0, \ldots, 10\right\},\,\text{size} = \text{nrow}(\bm{x}_{\text{train}}),\, \text{replace} = \text{T})$
    \State $\dataset_{\text{in-bag}} \gets \text{subset}(\dataset_{\text{train}},\,\text{rows} = \text{which}(w > 0))$
    \State $w \gets \text{subset}(w,\, \text{which}(w > 0))$
    \State $\text{node\_assignments} \gets \text{rep}(1,\,\text{times} = \text{nrow}(\bm{x}_{\text{in-bag}}))$
    \State $\text{nodes\_to\_split} \gets \{1\}$
     \While {$\text{nodes\_to\_split} \neq \emptyset$}
     \For{$\text{node} \in \text{nodes\_to\_split}$}
       \State $\text{n\_try} \gets 1$
       \State $\text{node\_rows} \gets \text{which}(\text{node\_assignments} \equiv \text{node})$
       \State $\text{node\_cols} \gets \text{sample}(\text{from} = \left\{1, \ldots, \text{ncol}(\bm{x})\right\},\, \text{size} = \text{mtry},\,\text{replace} = \text{F})$ \label{marker}
       \State $\dataset_{\text{node}} \gets \text{subset}(\dataset_{\text{in-bag}},\,\text{rows} = \text{node\_rows},\,\text{columns} = \text{node\_cols})$
       \State $\beta \gets \text{newt\_raph}(\dataset_{\text{node}},\, \text{weights} = \text{subset}(w, \text{node\_rows}),\, \text{max\_iter} = 1)$
       \State $\eta \gets \bm{x}^T_{\text{node}} \times \beta$
       \State $\mathcal{C} \gets \text{sample}(\text{from} = \text{unique}(\eta),\, \text{size} = \text{n\_split},\,\text{replace} = \text{F})$
       \State $c \gets \argmax_{c^* \in \mathcal{C}} \left\{ \text{log\_rank\_stat}(\eta, c^*) \right\}$
       \If{$\text{log\_rank\_stat}(\eta, c) \geq \text{split\_min\_stat}$}
         \State $\mathcal{T} \gets \text{add\_node}(\mathcal{T},\, \text{name} = \text{node},\, \text{beta} = \beta,\, \text{cutpoint} = c)$
         \State \LeftComment{0}{Right node logic omitted for brevity (identical to left node logic)}
         \State $\text{node\_left\_name} \gets \text{max}(\text{node\_assignments}) + 1$
         \State $\text{node\_left\_rows} \gets \text{subset}(\text{node\_rows},\,\text{which}(\eta \leq c))$
         \State $\text{subset}(\text{node\_assignments}, \text{node\_left\_rows}) \gets \text{node\_left\_name}$
         \If{$\text{is\_splittable}(\text{subset}(\text{node\_assignments}, \text{node\_left\_rows}))$}
           \State $\text{nodes\_to\_split} \gets \text{nodes\_to\_split} \cup \text{node\_left\_name}$
         \Else
           \State $\mathcal{T} \gets \text{add\_leaf}(\mathcal{T},\, \text{data} = \text{subset}(\dataset_{\text{node}},\,\text{rows} = \text{node\_left\_rows}))$
         \EndIf
       \ElsIf{$\text{n\_try} \leq \text{n\_retry}$}
         \State $\text{n\_try} \gets \text{n\_try} + 1$
         \State \Goto{marker}
       \Else
         \State $\mathcal{T} \gets \text{add\_leaf}(\mathcal{T},\, \text{data} =\dataset_{\text{node}})$
       \EndIf
       \State $\text{nodes\_to\_split} \gets \text{nodes\_to\_split} \setminus \{\text{node}\}$
     \EndFor
  \EndWhile
  \State \Return $\mathcal{T}$
  \end{algorithmic}
\end{algorithm}
\spacingset{1.5}

\subsection{Negation variable importance} \label{sec:negation_vi}

This Section introduces negation VI, which is similar to permutation VI in that it measures how much a model’s prediction error increases when a variable’s role in the model is de-stabilized. Specifically, negation VI measures the increase in an oblique RF's prediction error after flipping the sign of all coefficients linked to a variable (\ie, negating them). As negating a coefficient effectively flips decision boundaries around the corresponding predictor's axis, scaling numeric predictors to have a mean of zero and standard deviation of one is recommended.\footnote{The \texttt{aorsf} package automatically scales numeric inputs to a mean of zero and standard deviation of one.} For the current study, we use Harrell's concordance (C)-statistic \citep{harrell1982evaluating} to measure change in prediction error when computing negation VI.

% A motivating problem for negation VI is that of correlated, numeric predictors. When two predictors are correlated and only one of them is relevant for prediction of an outcome, VI methods based on permutation are known to have upward bias in estimating the importance of the irrelevant predictor \citep{hooker2021unrestricted}. Permutation may distort the joint distribution of predictors, which may in turn increase a model's estimated prediction error due to extrapolation. On the other hand, negating coefficients modifies the decision rule rather than modifying the data. Thus, we hypothesize that negation VI can be more effective at identifying important predictors for correlated, numeric predictors.

To demonstrate negation VI, consider a classification task where the goal is prediction of penguin species (chinstrap, gentoo, or adelie) based on bill length and bill depth \citep{palmerpenguins_rpack}. Scaling these predictors to be centered at 0, we find oblique decision boundaries defined by linear combinations of bill length and bill depth correctly classify most of the data (Figure \ref{fig:negation}, Panel A). Permuting the values of bill length leads to several mis-classified observations, suggesting that bill length is an important predictor (Figure \ref{fig:negation}, Panel B). However, inspecting the permuted data shows that a number of observations moved to a region in the predictor space where there were previously no observations. Moving data to unobserved or perhaps unobservable regions of the predictor space may cause extrapolation error, which \citet{hooker2021unrestricted} identified as a cause of bias in permutation importance. Negating the coefficients for bill length in the linear combinations that define our decision boundaries causes the boundaries to flip across bill length's axis (Figure \ref{fig:negation}, Panel C). This leads to several mis-classified observations, suggesting that bill length is an important predictor without distorting the joint distribution of bill length and bill depth.

Negation VI is an extension of ``anti VI", a VI technique for axis-based trees which became the default VI method for \texttt{randomForestSRC} in version 2.14.0. Anti VI reverses the direction of all decision nodes that use a specific variable, and then reassesses the ensemble prediction error. So, if an axis-based decision rule were defined as $x > 5 \Rightarrow$ send data to right node, the decision rule would become $x > 5 \Rightarrow$ send data to left node when computing the importance of a predictor $x$. Put in a way that makes the connection to negation VI more explicit, the `noised up' decision rule can be written as $-x > 5 \Rightarrow$ send data to right node.

<<negation, fig.height=4, fig.width=12, fig.cap="Demonstration of negation and permutation importance for a single oblique tree. Panel A shows the original data and decision boundaries. Panel B shows the data with permuted bill length values. Panel C shows the decision boundaries after negating coefficients for bill length. Permutation and negation both show that bill length is an important predictor, but permuting bill length distorts its joint distribution with bill depth.">>=

p1 <- ggplot(data = penguins_scaled) +
  aes(x = x, y = y) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_polygon(data=poly1, fill=cols[1], color='black', alpha=0.2) +
  geom_polygon(data=poly2, fill=cols[2], color='black', alpha=0.2) +
  geom_polygon(data=poly3, fill=cols[3], color='black', alpha=0.2) +
  theme_minimal() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  labs(y = "Bill depth, scaled",
       x = "") +
  coord_cartesian(ylim = c(-2.1, 2.1),
                  xlim = c(-3.1, 3.1)) +
  theme(panel.grid = element_blank(),
        legend.position = '',
        text = element_text(size = 12))

poly1_flip <- mutate(poly1, x = -x)
poly2_flip <- mutate(poly2, x = -x)
poly3_flip <- mutate(poly3, x = -x)

p2 <- ggplot(data = penguins_scaled) +
  aes(x = x, y = y) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_polygon(data=poly1_flip, fill=cols[1], color='black', alpha=0.2) +
  geom_polygon(data=poly2_flip, fill=cols[2], color='black', alpha=0.2) +
  geom_polygon(data=poly3_flip, fill=cols[3], color='black', alpha=0.2) +
  theme_minimal() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  labs(x = "", y = "") +
  coord_cartesian(ylim = c(-2.1, 2.1),
                  xlim = c(-3.1, 3.1)) +
  theme(panel.grid = element_blank(),
        legend.position = '',
        text = element_text(size = 12),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())


penguins_permuted <- penguins_scaled %>%
  mutate(x = sample(x, n(), replace = FALSE))

p3 <- ggplot(data = penguins_permuted) +
  aes(x = x, y = y) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_polygon(data = poly1, fill = cols[1], color = 'black', alpha = 0.2) +
  geom_polygon(data = poly2, fill = cols[2], color = 'black', alpha = 0.2) +
  geom_polygon(data = poly3, fill = cols[3], color = 'black', alpha = 0.2) +
  # geom_text(aes(x = -2, y = -1.5, label = "Impossible?"),
            # size = 4, check_overlap = TRUE) +
  theme_minimal() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = cols) +
  labs(x = "Bill length, scaled") +
  coord_cartesian(ylim = c(-2.1, 2.1),
                  xlim = c(-3.1, 3.1)) +
  theme(panel.grid = element_blank(),
        legend.position = '',
        text = element_text(size = 12),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())

ggarrange(tag_facet(p1, tag_pool = "A", vjust = 2),
          tag_facet(p3, tag_pool = "B", vjust = 2),
          tag_facet(p2, tag_pool = "C", vjust = 2),
          nrow = 1)


@


Negation VI has several strengths. First, it generalizes to any oblique RF (\ie, not just RSFs) using any valid error function, making it both general and flexible.\footnote{The \texttt{aorsf} package enables customized functions to be applied in lieu of the default C-statistic.} Second, negation is non-random, making it reproducible without setting a random seed and making it slightly faster than permutation importance. Fourth, since negation VI does not permute variables, the analyst need not worry about impossible combinations of predictors that may occur when one predictor is randomly permuted, such as having a negative status for type 2 diabetes and having Hemoglobin A1c level $\geq$ 6.5\% (a value indicative of type 2 diabetes) as a result of randomly permuting the values of Hemoglobin A1c. However, in scenarios where decision boundaries have symmetry around the origin of the predictor space (\eg, all positive cases lie in a circle centered at the origin, with negative cases sitting outside the circle), negation importance will be less effective than permutation.


\section{Numeric experiments} \label{sec:numeric}

<<>>=
r_version <- paste(version$major, version$minor, sep = '.')
@

Sections \ref{sec:bm_pred} and \ref{sec:bm_vi} present the design of numerical experiments examining the accelerated oblique RSF and negation VI, respectively. \secref{sec:bm_compute} summarizes our approach to evaluating computational efficiency of learning algorithms, with a focus on the accelerated oblique RSF and other RSF implementations. \secref{sec:computing} provides details on computation and code.

\subsection{Benchmark of prediction accuracy} \label{sec:bm_pred}

The aim of this numeric experiment is to evaluate the prediction accuracy of the accelerated oblique RSF compared to its predecessor (the oblique RSF from the \texttt{obliqueRSF} R package) and to several other machine learning algorithms. Inferences drawn from this experiment include equivalence and inferiority tests based on Bayesian linear mixed models.

\subsubsection{Learners} \label{sec:learners}

We consider four classes of learners: RSFs (both axis-based and oblique), boosting ensembles, regression models, and neural networks. Specific learners from each class are summarized in \tabref{tab:learners}. To facilitate fair comparisons, tuning parameters were harmonized within each class. For example, for RSF learners, we set the minimum node size (a parameter shared by all RSF learners) as 10. Additionally, for RSF learners, the number of randomly selected predictors was the square root of the total number of predictors rounded to the nearest integer, and the number of trees in the ensemble was 500 (a common default value for the number of trees). For boosting, regression, and neural network learners, nested cross-validation was applied to tune relevant model parameters. Nested cross-validation includes an `inner' cross-validation loop to evaluate different specifications of tuning parameters and an `outer' loop that evaluates the tuned model, providing an unbiased estimate of the underlying model and its tuning procedure. Tuning for boosting models included identifying the number of steps to complete. The maximum number of steps was 5000, the learning rate was fixed at 0.01, and early stopping was applied if there was no improvement in cross-validated negative log-likelihood for 25 steps. For regression models, tuning was used to identify the magnitude of penalization. For neural networks, the number of layers (\ie, length) and number of nodes in the layers (\ie, width) was tuned, while dropout rate was fixed at 10\%, batch size was fixed at 32 observations, and the rectified linear unit activation function was applied. In addition, neural networks completed a maximum of 500 epochs, with possible early stopping based on prediction accuracy in a validation set comprising 25\% of its training data..

\newgeometry{margin=1cm} % modify this if you need even more space
\begin{landscape}
\spacingset{1}
\begin{table}[h!]
\centering
\begin{tabular}{p{2cm} | p{3cm} p{4cm} p{12cm}}
 \hline
 Learner Class & Software & Learners & Description \\ [0.5ex]
 \hline\hline
 \multicolumn{3}{l}{\textit{Random Survival Forests}}\\
 \hline\hline

 Axis based &

 \href{https://www.randomforestsrc.org/index.html}{\texttt{RandomForestSRC}} \newline
 \href{https://CRAN.R-project.org/package=ranger}{\texttt{ranger}} \newline
 \href{http://party.r-forge.r-project.org/}{\texttt{party}} \newline
 \href{https://github.com/whcsu/rotsf}{\texttt{rotsf}} \newline
 \href{https://github.com/whcsu/rsfse}{\texttt{rsfse}} &

 \texttt{rsf-standard} \newline
 \texttt{rsf-extratrees} \newline
 \texttt{cif-standard} \newline
 \texttt{cif-rotate} \newline
 \texttt{cif-spacextend} &

 \texttt{rsf-standard} grows survival trees following Leo Breiman's random forest algorithm with cut-points selected to maximize a log-rank statistic. \texttt{rsf-extratrees} grows survival trees with randomly selected predictors and cut-points. \texttt{cif-standard} uses conditional inference. \texttt{cif-rotate} extends \texttt{cif-standard} by applying principal component analysis to random subsets of data prior to growing each survival tree. \texttt{cif-spacextend} derives new predictors for each tree in the ensemble, separately. \\ \hline


 Oblique &


 \href{https://CRAN.R-project.org/package=obliqueRSF}{\texttt{obliqueRSF}} \newline
 \href{https://bcjaeger.github.io/aorsf/}{\texttt{aorsf}} &

 \texttt{obliqueRSF-net} \newline
 % \texttt{aorsf-net} \newline
 \textcolor{purple}{\texttt{aorsf-fast}} \newline
 \texttt{aorsf-cph} \newline
 \texttt{aorsf-extratrees} &


 Oblique survival trees following Leo Breiman's random forest algorithm. Linear combinations of inputs are derived using \texttt{glmnet} in \texttt{obliqueRSF-net}, using Newton Raphson scoring for the Cox partial likelihood function in \texttt{aorsf-fast} (1 iteration of scoring) and \texttt{aorsf-cph} (up to 20 iterations), and chosen randomly from a uniform distribution in \texttt{aorsf-extratrees}. Cut-points are selected to maximize a log-rank statistic. \\

 \hline\hline
 \multicolumn{3}{l}{\textit{Boosting ensembles}}\\
 \hline\hline

 Trees &

 \href{https://xgboost.readthedocs.io/en/stable/#}{\texttt{xgboost}} &

 \texttt{xgboost-cox} \newline
 \texttt{xgboost-aft} &

 \texttt{xgboost-cox} maximizes the Cox partial likelihood function, whereas \texttt{xgboost-aft} maximizes the accelerated failure time likelihood function. Nested cross validation (5 folds) is applied to tune the number of trees. The minimum number of observations in a leaf node was 10, the maximum depth of trees was 6, and $\sqrt{p}$ variables were considered randomly for each split, where $p$ is the number of predictors. \\

 \hline\hline
 \multicolumn{3}{l}{\textit{Regression models}}\\
 \hline\hline

 Cox Net &

 \texttt{glmnet} &

 \texttt{glmnet-cox} &

 The Cox proportional hazards model is fit using an elastic net penalty. Nested cross validation (5 folds) is applied to tune penalty terms.\\

 \hline\hline
 \multicolumn{3}{l}{\textit{Neural networks}}\\
 \hline\hline

 Cox Time &

 \href{https://raphaels1.github.io/survivalmodels/}{\texttt{survivalmodels}} &

 \texttt{nn-cox} &

 A neural network based on the proportional hazards model with time-varying effects. Nested cross-validation was applied to select the number of layers (from 1 to 8), the number of nodes in each layer (from $\sqrt{p}$/2 to $\sqrt{p}$), and the number of epochs to complete (up to 500). A drop-out rate of 10\% was applied during training.   \\
 \hline

\end{tabular}
\caption{Learning algorithms assessed in numeric studies. \textcolor{purple}{\texttt{aorsf-fast}} is the accelerated oblique random survival forest (see Algorithm \ref{alg:aorsf}), and each of the additional learners are compared to \textcolor{purple}{\texttt{aorsf-fast}} in numeric studies.}
\label{tab:learners}
\end{table}

\end{landscape}
\restoregeometry
\spacingset{1.5}


\subsubsection{Evaluation of prediction accuracy} \label{sec:prediction_accuracy}

Our primary metric for evaluating the accuracy of predicted risk is the integrated and scaled Brier score \citep{graf1999assessment}, a proper scoring rule that combines discrimination and calibration in one value and improves interpretability by adjusting for a benchmark model \citep{kattan2018index}. Consider a testing data set:
$$\dataset_{\text{test}} = \left\{ (T_i, \delta_i, x_{i}) \right\}_{i=1}^{N_{\text{test}}}.$$
Let $\widehat{S}(t \mid x_i)$ be the predicted probability of survival up to a given prediction time of $t > 0$.
 For observation $i$ in $\dataset_{\text{test}}$, let $\widehat{S}(t \mid \bm{x}_i)$ be the predicted probability of survival up to a given prediction time of $t > 0$. Define \begin{align*}
\bstat = \frac{1}{\ntest} \sum_{i=1}^{\ntest} &\{ \widehat{S}(t \mid \bm{x}_i)^2 \cdot I(T_i \leq t, \delta_i = 1) \cdot \widehat{G}(T_i)^{-1} \\ &+ [1-\widehat{S}(t \mid \bm{x}_i)]^2 \cdot I(T_i > t) \cdot \widehat{G}(t)^{-1}\}
\end{align*} where $\widehat{G}(t)$ is the Kaplan-Meier estimate of the censoring distribution. As $\bstat$ is time dependent, integration over time provides a summary measure of performance over a range of plausible prediction times. The integrated $\bstat$ is defined as \begin{equation}
\bsbar = \frac{1}{t_2 - t_1}\int_{t_1}^{t_2} \widehat{\text{BS}}(t) dt.
\end{equation} In our results, $t_1$ and $t_2$ are the 25th and 75th percentile of event times, respectively. $\bsbar$, a sum of squared prediction errors, can be scaled to produce a measure of explained residual variation (\ie, an $R^2$ statistic) by computing \begin{equation}
R^2 = 1 - \frac{\bsbar}{\bskap}
\end{equation} where $\bskap$ is the integrated Brier score when a Kaplan-Meier estimate for survival based on the training data is used as the survival prediction function $\widehat{S}(t)$. We refer to this $R^2$ statistic as the index of prediction accuracy (IPA) \citep{kattan2018index}.

Our secondary metric for evaluating predicted risk is the time-dependent concordance (C)-statistic. We compute the first time-dependent C-statistic proposed by \citet[][Equation~3]{blanche2013estimating}, which is interpreted as the probability that a risk prediction model will assign higher risk to a case (\ie, an observation with $T \leq t$ and $\delta = 1$) versus a non-case (\ie, an observation with $T > t$). Similar to the IPA, observations with $T \leq t$ and $\delta = 0$ only contribute to inverse probability of censoring weights for the time-dependent C-statistic.

Both the IPA and time-dependent C-statistic generally take values between 0 and 1. To avoid presenting an excessive amount of leading zeroes in our tables, figures, and text, we scale both the IPA and time-dependent C-statistic by 100. For example, we present a value of 25 if the IPA is 0.25, 87 if the time-dependent C-statistic is 0.87, and present 10.2 if the difference between two IPA values is 0.102

\subsubsection{Data sets}

We used a collection of \Sexpr{n_data_sets} data sets containing a total of \Sexpr{n_risk_tasks} risk prediction tasks (tasks per data set ranged from one to four). Participant-level data from the GUIDE-IT and SPRINT clinical trials and the ARIC, MESA, and JHS community cohort studies was obtained from the National Institute of Health Biologic Specimen and Data Repository Coordinating Center (BioLINCC). Designs and protocols for these studies have been made available \citep{aric1989atherosclerosis, bild2002multi, felker2017effect, sprint2015randomized, taylor2005toward}. All other datasets were publicly available and obtained through R packages (see Appendix A.1 in Supplementary Materials). Across all prediction tasks, the number of observations ranged from \Sexpr{nrow_min} to \Sexpr{nrow_max} (median: \Sexpr{nrow_median}), the number of predictors ranged from \Sexpr{ncol_min} to \Sexpr{ncol_max} (median: \Sexpr{ncol_median}), and the percentage of censored observations ranged from \Sexpr{pcens_min} to \Sexpr{pcens_max} (median: \Sexpr{pcens_median}) (\tabrefAppendix{tab:datasets} in Supplementary Materials).

\subsubsection{Monte-Carlo cross validation}

For each risk prediction task, we completed \Sexpr{bm_pred_n_runs} runs of Monte-Carlo cross validation. In each run, we used a random sample containing 50\% of the available data for training and the remaining 50\% for testing each of the learners described in \secref{sec:learners}. Then, for each learner, we computed the IPA and time-dependent C-statistic. If any learner failed to obtain predictions on any particular split of data\footnote{For example, when the prediction task was to predict risk of death in the ACTG 320 clinical trial (26 events total), some splits did not leave enough events in the training data to fit complex learners such as neural networks}, the results for that split were omitted from downstream analyses for all learners.


\subsubsection{Statistical analysis}

After collecting data from \Sexpr{bm_pred_n_runs} replications of Monte-Carlo cross validation for the \Sexpr{n_learners} learners in all \Sexpr{n_risk_tasks} risk prediction tasks, we analyzed the resulting \Sexpr{n_obs_model} observations of IPA and, separately, time-dependent C-statistic, using a Bayesian linear mixed model. Our approach follows the ideas described by \citet{benavoli2017time} and \citet{tidymodels}, who developed guidelines on making statistical comparisons between learners using Bayesian models. Specifically, we fit two models: $$\text{IPA} = \widehat{\gamma}_0 + \widehat{\gamma} \cdot \text{learner} + (1\,|\, \text{data/run}) $$ and $$\text{C-stat} = \widehat{\gamma}_0 + \widehat{\gamma} \cdot \text{learner} + (1\,|\, \text{data/run}).$$ Random intercepts for specific splits of data (\ie, \texttt{run} in the model formula) were nested within datasets. The intercept, $\widehat{\gamma}_0$, was the expected value of the outcome using \texttt{aorsf-fast}, making the coefficients in $\widehat{\gamma}$ the expected differences between \texttt{aorsf-fast} and other learners. Default priors from \texttt{rstanarm} were applied for model fitting \citep{rstanarm}.

\paragraph{Hypothesis testing} For both the IPA and time-dependent C-statistic, we conducted equivalence and inferiority tests based on a 1 point region of practical equivalence. More specifically, we concluded that two learners had practically equivalent IPA or time-dependent C-statistic if there was a 95\% or higher posterior probability that the absolute difference in the relevant metric was less than 1. We concluded that one learner was weakly superior when there was $\geq$ 95\% posterior probability that the absolute difference in the relevant metric was non-zero, and concluded superiority when when there was $\geq$ 95\% posterior probability that the absolute difference in the relevant metric was 1 or more.

<<>>=

n_tasks_complete <- bm_pred_clean %>%
  getElement("data") %>%
  distinct(data, run) %>%
  nrow()

n_tasks_expected <- max(bm_pred_clean$data$run) *
  length(unique(bm_pred_clean$data$data))

# bm_pred_failed_description <- "Nothing to say"

bm_pred_failed_description <-
  bm_pred_clean$omit %>%
  mutate(
    model = recode(model, !!!deframe(model_key)),
    data = recode(data, !!!deframe(data_key[,c('data', 'label')]))
  ) %>%
  split(f = list(.$model, .$data)) %>%
  map(
    ~ glue(
    "On run {glue_collapse(.x$run, sep = ', ', last = ' and ')} \\
     for the {.x$data[1]} data, the \\texttt{{{.x$model[1]}}} \\
     learner encountered an error during its fitting procedure"
    )
  ) %>%
  reduce(paste, sep = '. ')

n_wins_aorsf <- bm_pred_viz %>%
  getElement('ibs_scaled') %>%
  getElement('rankings') %>%
  filter(model == 'aorsf_fast') %>%
  pull(n_wins)

ibs_scaled_mean_aorsf <- bm_pred_viz %>%
  getElement('ibs_scaled') %>%
  getElement('smry') %>%
  filter(model == 'aorsf_fast',
         data == 'Overall') %>%
  mutate(eval = table_value(eval * 100)) %>%
  pull(eval)

ibs_scaled_aorsf_won_by <- bm_pred_viz %>%
  getElement('ibs_scaled') %>%
  getElement('diffs') %>%
  filter(data == 'Overall') %>%
  transmute(percent = table_value(pdiff),
            absolute = table_value(100*adiff)) %>%
  as.list()

ibs_scaled_aorsf_equiv <- bm_pred_model_viz %>%
  getElement('data') %>%
  filter(metric == 'ibs_scaled',
         model == 'aorsf-cph') %>%
  pull(prob_equiv)

ibs_scaled_aorsf_sup_min <- bm_pred_model_viz %>%
  getElement('data') %>%
  ungroup() %>%
  filter(metric == 'ibs_scaled',
         !str_detect(model, '^aorsf|^oblique')) %>%
  arrange(desc(median)) %>%
  slice(1) %>%
  select(model, prob_super_duper) %>%
  mutate(model = as.character(model)) %>%
  as.list()

@


\subsection{Benchmark of variable importance} \label{sec:bm_vi}

The aim of this experiment is to evaluate negation VI and similar VI methods based on how well they can discriminate between relevant and irrelevant variables, where relevance is defined by having a relationship with the simulated outcome. We consider methods that are intrinsic to the oblique RF (\eg, ANOVA VI), those that are intrinsic to the RF (\eg, permutation VI), and those that are model-agnostic (\eg, SHAP VI). VI methods with unavailable or still developing software were not included.\footnote{Although the \texttt{party} package implements the approach to VI developed by \citet{strobl2007bias}, the developers of the \texttt{party} package note that the implementation of this approach for survival outcomes is ``extremely slow and experimental'' as of version 1.3.10. Therefore, it is not incorporated in the current simulation study.}

\subsubsection{Variable importance techniques}

We compute permutation VI for axis based RSFs using the \texttt{randomForestSRC} package. We compute ANOVA VI, negation VI, and permutation VI for oblique RSFs using the \texttt{aorsf} package. For ANOVA VI, we applied a p-value threshold of 0.01, following the threshold recommended by \citet{menze2011oblique}. We compute SHAP VI for boosted tree models using the \texttt{xgboost} package \citep{xgboost}, which incorporates the tree SHAP approach proposed by \citet{lundberg2018consistent}.

% We also compute SHAP VI for accelerated oblique RSFs using the \texttt{fastshap} package.

\subsubsection{Variable types}

We considered five classes of predictor variables, with each class characterized by its variables' relationship to a right-censored outcome on the log-hazard scale. Specifically, \begin{itemize}
\item \textit{irrelevant} variables had no relationship with the outcome.
\item \textit{main effect} variables had a linear relationship to the outcome on the log-hazard scale.
\item \textit{non-linear effect} variables had a non-linear relationship to the outcome. A normally distributed variable $x$ was generated with a linear relationship to the outcome on the log-hazard scale, then $\tilde{x} = \text{sin}(a \cdot \pi \cdot x)$ was retained for modeling. The constant $a$ varied uniformly from 0.125 to 0.25.
\item \textit{combination effect} variables were formed by linear combinations of three other variables. While their combination was linearly related to the outcome on the log-hazard scale, each of the three variables contributing to the combination had no relation to the outcome.
\item \textit{interaction effect} variables were related to the outcome by multiplicative interaction with one other variable, which could have been a main effect, non-linear effect, or combination effect variable.
\end{itemize}

\subsubsection{Simulated data} \label{sec:data_sim}

We initiated each set of simulated data with a random draw of size $n$ from a $p$-dimensional multivariate normal distribution, yielding $n$ observations of $p$ predictors. Each of $p$ predictor variables had a mean of zero, standard deviation of 1, and correlation with other predictor variables drawn at random between a lower and upper boundary. A time-to-event outcome with roughly 45\% of observations censored was generated using the \texttt{simsurv} package \citep{simsurv, simsurv_paper} from a Weibull distribution with scale parameter (\ie, \texttt{lambdas}) equal to 0.1 and shape parameter (\ie, \texttt{gammas}) equal to 1.5. The full predictor matrix (\ie, including interactions, non-linear mappings, and combinations) was used to generate the outcome. Interactions, non-linear mappings, and combinations were dropped from the predictor matrix after the outcome was generated so that VI techniques could be evaluated based on their ability to detect these effects.

\subsubsection{Parameter specifications}

<<>>=

n_obs <- bm_vi_viz$smry %>%
  ungroup() %>%
  distinct(n_obs) %>%
  drop_na() %>%
  filter(n_obs > 0) %>%
  pull(n_obs) %>%
  glue_collapse(sep = ', ', last = ', and ')

pred_corr <- bm_vi_viz$smry %>%
  ungroup() %>%
  distinct(pred_corr_max) %>%
  drop_na() %>%
  filter(pred_corr_max < 1) %>%
  pull(pred_corr_max) %>%
  glue_collapse(sep = ', ', last = ', and ')

s <- mean_ci(clincalc_r2)*100

clincalc_r2_sexpr <- table_glue(
  "{s[['y']]} ({s[['ymin']]}, {s[['ymax']]})"
)

@


Parameters that varied in the current simulation study included the number of observations (\Sexpr{n_obs}) and the absolute value of the maximum correlation between predictors (\Sexpr{pred_corr}). Parameters that remain fixed throughout the study included the number of predictors in each class (15) and the effect size of each predictor (one standard deviation increase associated with a 64\% increase in relative risk). Using this design for simulated data, the Heller explained relative risk (95\% confidence interval) of our covariates was \Sexpr{clincalc_r2_sexpr} \citep{heller2012measure} with 2,500 observations.

\subsubsection{Evaluation of variable importance}

We compared VI techniques based on their discrimination (\ie, C-statistic) between relevant and irrelevant variables. Specifically, we generated a binary outcome for each predictor variable based on its relevance (\ie, the binary outcome is 1 if the variable is relevant, 0 otherwise). Treating VI as if it were a ‘prediction’ for these binary outcomes yields a C-statistic which may be interpreted as the probability that the VI technique will rank a relevant variable higher than an irrelevant variable \citep{harrell1982evaluating}.

\subsection{Benchmark of computational efficiency} \label{sec:bm_compute}

The aim of this numeric experiment is to evaluate the computational efficiency of the accelerated oblique RSF compared to its predecessor (the oblique RSF from the \texttt{obliqueRSF} R package) and to several other machine learning algorithms.

\subsubsection{Evaluation of computational efficiency}

For each learner discussed in \secref{sec:learners} and for each of the \Sexpr{n_risk_tasks} risk prediction tasks analyzed in \secref{sec:bm_pred}, we tracked the amount of time required to fit a prediction model (including time used to tune parameters) and compute predicted risk.

We performed additional benchmarks on the time required to fit 500 trees using \texttt{aorsf}, \texttt{randomForestSRC}, and \texttt{ranger}. The learners that represented these R packages were \texttt{aorsf-fast}, \texttt{rsf-standard}, and \texttt{rsf-extratrees}, respectively. To allow for controlled comparisons of computational efficiency with varying dimensions of training data, we used the same process to simulate data as described in \secref{sec:data_sim}, varying the number of observations from 100 to 10000 and the number of predictors from 10 to 1000. The minimum node size of trees in this experiment was dynamically set as the nearest integer to the number of observations in the training set divided by 10.

\subsection{Computational details} \label{sec:computing}

All analyses were conducted using R version \Sexpr{r_version} with version 0.0.4 of the \texttt{aorsf} \citep{jaeger2022aorsf} package. Analyses were coordinated with assistance from the \texttt{targets} package \citep{targets}. To standardize comparisons of computational efficiency, all learners and VI techniques used up to 4 processing units.

% All code used in the current study is available at \href{https://github.com/bcjaeger/aorsf-bench}{https://github.com/bcjaeger/aorsf-bench},

\section{Results} \label{sec:results}

In \secref{sec:results_pred}, \secref{sec:results_vi}, and \secref{sec:results_computing}, we present results from the benchmark or prediction accuracy, the simulation study of VI, and the benchmark of computational efficiency, respectively.

\subsection{Prediction accuracy} \label{sec:results_pred}

A full summary of all results presented in this Section is provided in \tabrefAppendix{tab:bm_pred_all} in Supplementary Materials. In total, \Sexpr{n_tasks_complete} out of \Sexpr{n_tasks_expected} Monte-Carlo cross validation runs were completed. \Sexpr{bm_pred_failed_description}.

\paragraph{Index of prediction accuracy}

Compared to learners that were not oblique RSFs, \texttt{aorsf-fast} had the highest IPA in \Sexpr{n_wins_aorsf} out of \Sexpr{n_risk_tasks} risk prediction tasks, with an overall mean IPA of \Sexpr{ibs_scaled_mean_aorsf} (Figure \ref{fig:bm_pred_viz_ibs}). Compared to the learner with the second highest mean IPA (\texttt{\Sexpr{ibs_scaled_aorsf_sup_min$model}}), \texttt{aorsf-fast}'s mean was \Sexpr{ibs_scaled_aorsf_won_by$absolute} points higher, a relative increase of \Sexpr{ibs_scaled_aorsf_won_by$percent}\%. The posterior probability of \texttt{aorsf-fast} and \texttt{aorsf-cph} having practically equivalent expected IPA was \Sexpr{ibs_scaled_aorsf_equiv}, and the posterior probability of \texttt{aorsf-fast} having a superior IPA to other learners ranged from \Sexpr{ibs_scaled_aorsf_sup_min$prob_super_duper} (versus \texttt{\Sexpr{ibs_scaled_aorsf_sup_min$model}}) to $>$0.999 (versus several other learners; see Figure \ref{fig:bm_pred_model_viz_ibs})

<<bm_pred_viz_ibs, fig.height=9, fig.width=9, fig.cap="Index of prediction accuracy in multiple risk prediction tasks. Text appears in tasks where the accelerated oblique random survival forest obtained the highest score, showing absolute and relative improvement over the second best learner. Since this figure is intended to compare \\texttt{aorsf-fast} to learners that are not oblique random survival forests, \\texttt{aorsf-cph}, \\texttt{aorsf-random}, and \\texttt{obliqueRSF-net} are not included.">>=
bm_pred_viz$ibs_scaled$fig
@


<<bm_pred_model_viz_ibs, fig.height=9, fig.width=9, fig.cap="Expected differences in index of prediction accuracy between the accelerated oblique random survival forest and other learning algorithms. A region of practical equivalence is shown by purple dotted lines, and a boundary of non-zero difference is shown by an orange dotted line at the origin.">>=
bm_pred_model_viz$fig$ibs_scaled
@

\paragraph{Time-dependent concordance statistic}

<<>>=

n_wins_aorsf <- bm_pred_viz %>%
  getElement('cstat') %>%
  getElement('rankings') %>%
  filter(model == 'aorsf_fast') %>%
  pull(n_wins)

cstat_mean_aorsf <- bm_pred_viz %>%
  getElement('cstat') %>%
  getElement('smry') %>%
  filter(model == 'aorsf_fast',
         data == 'Overall') %>%
  mutate(eval = table_value(eval * 100)) %>%
  pull(eval)

cstat_aorsf_won_by <- bm_pred_viz %>%
  getElement('cstat') %>%
  getElement('diffs') %>%
  filter(data == 'Overall') %>%
  transmute(percent = table_value(pdiff),
            absolute = table_value(100*adiff)) %>%
  as.list()

cstat_aorsf_equiv <- bm_pred_model_viz %>%
  getElement('data') %>%
  filter(metric == 'cstat',
         model == 'aorsf-cph') %>%
  pull(prob_equiv) %>%
  str_replace('\\>\\.', "$>$ 0.")

cstat_aorsf_sup_min <- bm_pred_model_viz %>%
  getElement('data') %>%
  ungroup() %>%
  filter(metric == 'cstat',
         !str_detect(model, '^aorsf'),
         model != 'obliqueRSF-net') %>%
  arrange(desc(median)) %>%
  slice(1) %>%
  select(model, prob_super_duper) %>%
  mutate(model = as.character(model)) %>%
  as.list()

@


Compared to learners that were not oblique RSFs, \texttt{aorsf-fast} had the highest time-dependent C-statistic in \Sexpr{n_wins_aorsf} out of \Sexpr{n_risk_tasks} risk prediction tasks, with an overall mean of \Sexpr{cstat_mean_aorsf} (Figure \ref{fig:bm_pred_viz_cstat}). Compared to the learner with the second highest mean C-statistic (\texttt{\Sexpr{cstat_aorsf_sup_min$model}}), \texttt{aorsf-fast}'s mean was \Sexpr{cstat_aorsf_won_by$absolute} points higher, a relative increase of \Sexpr{cstat_aorsf_won_by$percent}\%. The posterior probability of \texttt{aorsf-fast} and \texttt{aorsf-cph} having practically equivalent expected time-dependent C-statistics was \Sexpr{cstat_aorsf_equiv}, and the posterior probability of \texttt{aorsf-fast} having a superior time-dependent C-statistic versus other learners ranged from \Sexpr{cstat_aorsf_sup_min$prob_super_duper} (versus \texttt{\Sexpr{cstat_aorsf_sup_min$model}}) to $>$0.999 (versus several other learners; see Figure \ref{fig:bm_pred_model_viz_cstat})

<<bm_pred_viz_cstat, fig.height=9, fig.width=9, fig.cap="Time-dependent concordance statistic for the accelerated oblique random survival forest and other learning algorithms across multiple risk prediction tasks. Text appears in tasks where the accelerated oblique random survival forest obtained the highest concordance, showing the absolute and percent improvement over the second best learner. Since this figure is intended to compare \\texttt{aorsf-fast} to learners that are not oblique random survival forests, \\texttt{aorsf-cph}, \\texttt{aorsf-random}, and \\texttt{obliqueRSF-net} are not included.">>=
bm_pred_viz$cstat$fig
@


<<bm_pred_model_viz_cstat, fig.height=9, fig.width=9, fig.cap="Expected differences in time-dependent concordance statistic between the accelerated oblique random survival forest and other learning algorithms. A region of practical equivalence is shown by purple dotted lines, and a boundary of non-zero difference is shown by an orange dotted line at the origin.">>=
bm_pred_model_viz$fig$cstat
@

\subsection{Variable importance} \label{sec:results_vi}

<<>>=

n_wins_aorsf <- bm_vi_viz %>%
  getElement('rankings') %>%
  filter(model == 'aorsf-negate') %>%
  pull(n_wins)

n_vi_tasks <- sum(bm_vi_viz$rankings$n_wins)

cstat_mean_aorsf <- bm_vi_viz %>%
  getElement('smry') %>%
  filter(model == 'aorsf-negate',
         data == 'Overall_NA_NA') %>%
  mutate(value = table_value(value * 100)) %>%
  pull(value)

cstat_mean_anova <- bm_vi_viz %>%
  getElement('smry') %>%
  filter(model == 'aorsf-anova',
         data == 'Overall_NA_NA') %>%
  mutate(value = table_value(value * 100)) %>%
  pull(value)

cstat_mean_permute <- bm_vi_viz %>%
  getElement('smry') %>%
  filter(model == 'aorsf-permute',
         data == 'Overall_NA_NA') %>%
  mutate(value = table_value(value * 100)) %>%
  pull(value)

cstat_aorsf_won_by <- bm_vi_viz %>%
  getElement('diffs') %>%
  filter(data == 'Overall_NA_NA') %>%
  transmute(percent = table_value(pdiff),
            absolute = table_value(100*adiff)) %>%
  as.list()

cstat_vi_second_place <- bm_vi_viz %>%
  getElement('smry') %>%
  ungroup() %>%
  filter(data == 'Overall_NA_NA') %>%
  arrange(desc(value)) %>%
  slice(2) %>%
  transmute(model, value = table_value(100*value)) %>%
  as.list()

vi_times <- bm_vi_viz$times %>%
  as_inline('model', 'time') %>%
  map(as.numeric, unit = 'secs') %>%
  map(table_value) %>%
  set_names(str_replace(names(.), '-', '_'))

@

The three techniques that used `aorsf' to estimate VI were ranked first (\texttt{aorsf-negate}; $C = \Sexpr{cstat_mean_aorsf}$), second (\texttt{aorsf-anova}; $C = \Sexpr{cstat_mean_anova}$), and third (\texttt{aorsf-permute}; $C = \Sexpr{cstat_mean_permute}$) in overall mean C-statistic across all of the simulation scenarios, with \texttt{aorsf-negate} obtaining the highest C-statistic in \Sexpr{n_wins_aorsf} out of \Sexpr{n_vi_tasks} VI tasks (Figure \ref{fig:bm_vi_viz}). Among the four relevant variable classes, \texttt{aorsf-negate} had the highest mean C-statistic for main effects, combination effects, and non-linear effects, with the greatest advantage of using \texttt{aorsf-negate} occurring among non-linear and combination variables. Full results from the experiment are provided in \tabrefAppendix{tab:bm_vi} in Supplementary Materials. Computationally, ANOVA VI was faster than negation and permutation VI, with a median time of \Sexpr{vi_times$aorsf_anova} seconds versus \Sexpr{vi_times$aorsf_negate} and \Sexpr{vi_times$aorsf_permute} seconds, respectively.

<<bm_vi_viz, fig.height=10, fig.width=9, fig.cap="Concordance statistic for assigning higher importance to relevant versus irrelevant variables. Text appears in rows where negation importance obtained the highest concordance, showing absolute and percent improvement over the second best technique.">>=
bm_vi_viz$fig
@

\subsection{Computational efficiency} \label{sec:results_computing}

<<>>=

times <- bm_pred_time_viz %>%
  getElement("medians") %>%
  transmute(
    model,
    time,
    ratio = table_value(
      time / time[model == 'aorsf-fast']
    )
  ) %>%
  as_inline('model', c('time', 'ratio')) %>%
  set_names(str_replace(names(.), '-', '_'))

ms_diff <- round(abs(times$glmnet_cox$time - times$aorsf_fast$time)*1000)

@


In the analysis of \Sexpr{n_risk_tasks} risk prediction tasks, \texttt{aorsf-fast} was the second fastest learner overall, with a median time to develop a risk prediction model and compute predictions about \Sexpr{ms_diff} milliseconds longer than \texttt{glmnet-cox} (Figure \ref{fig:bm_pred_time}). Comparing median computing times, \texttt{aorsf-fast} was \Sexpr{times$obliqueRSF_net$ratio} times faster than its predecessor, \texttt{obliqueRSF-net}. In addition, \texttt{aorsf-fast} was \Sexpr{times$cif_standard$ratio}, \Sexpr{times$ranger_extratrees$ratio}, and \Sexpr{times$rsf_standard$ratio} faster than axis based forests grown using the \texttt{party}, \texttt{ranger}, and \texttt{randomForestSRC} packages, respectively.

<<bm_pred_time, fig.height=10, fig.width=9, fig.cap="Distribution of time taken to fit a prediction model and compute predicted risk. The median time, in seconds, is printed and annotated for each learner by a vertical line.">>=
bm_pred_time_viz$fig
@

In the analysis of time to fit 500 trees using simulated data, the \texttt{ranger} package exhibited the fastest computation times overall (Figure \ref{fig:bm_time_viz}). \texttt{aorsf} was the second fastest when the number of predictors was 10 or 100,  and \texttt{randomForestSRC} had similar computation time versus \texttt{aorsf} when 1000 predictors were present.

<<bm_time_viz, fig.height=4, fig.width=10, fig.cap="The expected time, in seconds, to fit an ensemble of 500 axis-based survival trees using the ranger or randomForestSRC package versus 500 oblique survival trees using the aorsf package. The ranger package is the most efficient overall, and aorsf appears to be relatively efficient in larger samples, particularly when 10 or 100 predictors are present in the training data. All three packages appear to scale linearly in computation time with the number of observations in the training data.">>=

bm_time_viz

@


\section{Discussion} \label{sec:discussion}

In this paper, we have developed two contributions to the oblique RSF: (1) the accelerated oblique RSF (\ie, \texttt{aorsf-fast}) and (2) negation VI. Our technique to accelerate the oblique RSF reduces the number of operations required to find linear combinations of inputs using a single iteration of Newton Raphson scoring, while our VI technique directly engages with coefficients in linear combinations of inputs to measure importance of individual variables. In numeric experiments, we found that \texttt{aorsf-fast} is approximately \Sexpr{times$obliqueRSF_net$ratio} times faster than its predecessor, \texttt{obliqueRSF-net}, with a practically equivalent C-statistic. We also found that negation VI, a technique to estimate VI using the oblique RSF, detected non-linear, combination, and main effects more effectively than three standard methods to estimate VI: permutation, ANOVA, and SHAP VI. Overall, we found that estimating VI using negation instead of ANOVA increased the C-statistic for ranking a relevant variable higher than an irrelevant variable by \Sexpr{cstat_aorsf_won_by$absolute}, a relative increase of \Sexpr{cstat_aorsf_won_by$percent}\%.

To understand potential differences in computational efficiency, we reviewed code in the \texttt{aorsf}, \texttt{randomForestSRC}, and \texttt{ranger} packages. We found differences in how survival outcome data are saved in leaf nodes. For each leaf node, \texttt{aorsf} stores data with one row per unique event time using training data that are stored in the leaf, whereas \texttt{randomForestSRC} and \texttt{ranger} store survival outcomes at a fixed grid of event times in each leaf. By default, \texttt{ranger} creates a grid that includes all event times in the training data. The grid strategy can cause higher computing time and memory usage when the grid of event times is large and a large number of leaf nodes are included in each tree, which can occur when minimum node size is small relative to the size of the training data. We kept minimum node size fixed in our benchmark of computational efficiency using real data, and dymanically increased minimum node size based on the size of the training set when we benchmarked computational efficiency using simulated data. Because of this decision, the \texttt{randomForestSRC} and \texttt{ranger} packages ran slower than \texttt{aorsf} in our benchmark of real data but not in the benchmark of simulated data.

\subsection{Implications of our results}

Accurate risk prediction models have the potential to improve healthcare by directing timely interventions to patients who are most likely to benefit. However, prediction models that cannot scale adequately to large databases or cannot be interpreted and explained will struggle to gain acceptance in clinical practice \citep{moss2022demystifying}. The current study advances the oblique RSF, an accurate risk prediction model, towards being accurate, scalable, and interpretable. The improved computational efficiency of the accelerated oblique RSF increases the feasibility of applying oblique RSFs in a wide range of prediction tasks. Faster model evaluation and re-fitting also improve diagnosis and resolution of model-based issues (\eg, model calibration deteriorates over time). The introduction of negation VI also advances interpretability. VI is intrinsically linked to model fairness, as it can be used to identify when protected characteristics such as race, religion, and sexuality are inadvertently used (either directly or through correlates of these characteristics) by a prediction model. Since negation VI engages with the coefficients used in linear combinations of variables, a major component of oblique RSFs, it may be more capable of diagnosing unfairness in oblique RSFs compared to permutation importance and model-agnostic VI techniques.

\subsection{Limitations and next steps}

While the current study advances the oblique RSF towards being scalable and interpretable, there remain several limitations that can be targeted in future studies. The validity of the Cox partial likelihood function depends on an assumption of non-informative censoring, therefore the oblique RSF could be improved by incorporating methods to account for informative censoring prior to identifying a linear combination of variables \citep{he2016proportional, cui2017consistency}.  The accelerated oblique RSF does not account for competing risks, and biased estimation of incidence may occur when competing risks are ignored. Thus, allowing the oblique RSF to account for competing risks is a high priority for future studies. In addition, the current study only considered data without missing values, only evaluated oblique RSFs that applied the log-rank statistic for node splitting, and only considered negation VI estimates based on Harrell's C-statistic. Few studies have developed strategies to deal with missing data while growing oblique survival trees. Prior studies have found that log-rank tests can be mis-informative when survival curves cross \citep{li2015statistical}, and that Harrell's C-statistic is dependent on the censoring distribution of the outcome \citep{uno2011c}. Thus, a second item is to expand the range of options available to users of the \texttt{aorsf} package, enabling them to apply strategies for imputation of missing values and use a broad range of statistical criteria while growing oblique survival trees. Last, \citet{cui2017consistency} found that estimating an inverse-probability weighted hazard function at each non-leaf node of a survival tree allows the RSF to converge asymptotically to the true survival function when some variables contribute both to the risk of the event and the risk of censoring, a scenario that is very likely in the analysis of medical data. The accelerated oblique RSF could incorporate this splitting technique by using Newton Raphson scoring to fit a model for the censoring distribution after which a weighted model could be fit to the failure distribution. This final item has the highest priority, as \citet{cui2017consistency} showed it is a requisite condition for consistency of axis-based survival trees in fairly general settings.

% Acknowledgements should go at the end, before appendices and references

\subsection{Conclusion}

Oblique RSFs have exceptional prediction accuracy and this study has shown how they can be fit with computational efficiency that rivals their axis-based counterparts. We have also introduced a general and flexible method to estimate VI with oblique RFs, and demonstrated its effectiveness for numeric, correlated predictors.

\section{Supplementary Materials}

Supplemental materials for the current analysis are available online.

\begin{description}

\item[Code] The code used to generate results in the current analysis are available online at \newline https://github.com/bcjaeger/aorsf-bench and provided as supplemental material (aorsf\_bench.zip).

\item[aorsf] The R package used to fit oblique RSFs in the current analysis is available online at \newline https://github.com/ropensci/aorsf and version 0.0.7.9000 is provided as supplemental material (aorsf\_package.zip).

\item[Appendix] Additional details on data used in the current analysis as well as additional details for numeric experiments (aorsf\_appendix.pdf).

\end{description}

\bibliographystyle{Chicago}

\bibliography{main}

\newpage

\appendix

\section*{Appendix}
\captionsetup{labelformat=AppendixTables}
\setcounter{table}{0}

\subsection*{Data sources}

<<>>=
data_key_tbl <- data_key %>%
  group_by(label) %>%
  mutate(nrow = min(nrow),
         ncol = min(ncol)) %>%
  ungroup()
@


\begin{enumerate}

<<results='asis'>>=

data_key_tbl %>%
  mutate(
    source = if_else(
      is.na(rpack),
      'BioLINCC',
      paste0('the \\texttt{', rpack, '} R package \\citep{', rpack, '}')),
    preproc = if_else(
      preproc == 'none',
      "No relevant pre-processing steps were taken",
      paste0("In pre-processing steps, ", preproc)
    )
  ) %>%
  glue_data(" \\item The ``{label}'' data \\citep{{{cite}}} were obtained from {source}. {preproc}") %>%
  unique() %>%
  paste(collapse = '. \n') %>%
  # removes just the first occurrence, which is not needed.
  str_remove(pattern = stringr::fixed('\\citep{censored}')) %>%
  paste0(".") %>%
  cat()

@

\end{enumerate}

<<>>=

data_key_tbl %>%
  select(label, nrow, ncol, outcome, nevent, pcens) %>%
  mutate(across(starts_with('p'), ~ .x * 100),
         across(starts_with("n"), as.integer),
         across(where(is.numeric), table_value),
         across(where(is.character), str_replace, '0.000', '0'),
         outcome = recode(
           outcome,
           'HF' = 'heart failure',
           'CVD death' = 'cardiovascular death'),
         outcome = str_to_title(outcome),
         outcome = recode(outcome,
                          'Gross 1m Usd' = 'Gross 1M USD',
                          "Aids Diagnosis" ='AIDS Diagnosis')) %>%
  kable(
    col.names = c(
      'Label',
      'N observations',
      'N predictors',
      'Outcome',
      "N Events",
      '% Censored'
    ),
    align = 'lcclcc',
    booktabs=TRUE,
    longtable=TRUE,
    format = 'latex',
    caption = "Data sets used for numeric experiments \\label{tab:datasets}"
  ) %>%
  collapse_rows(columns=1:3) %>%
  # pack_rows(index = indents,
  #           italic = TRUE,
  #           hline_before = TRUE,
  #           hline_after = TRUE) %>%
  kable_styling() %>%
  landscape()



@

\newpage

<<>>=

data_recoder <- data_key %>%
  transmute(data,
            label = paste0(label, "; ",
                           outcome, ", n = ", .data$nrow,
                           ", p = ", .data$ncol)) %>%
  deframe()

model_recoder <- model_key %>%
  deframe()

results_overall <- bm_pred_clean %>%
  getElement('data') %>%
  group_by(model) %>%
  summarize(
    across(
      .cols = c(cstat, ibs_scaled, time_fit, time_pred),
      .fns = list(mean = mean,
                  median = median,
                  sd = sd)
    )
  ) %>%
  mutate(data = 'Overall')

data_tbl <- bm_pred_clean %>%
  getElement('data') %>%
  group_by(model, data) %>%
  summarize(
    across(
      .cols = c(cstat, ibs_scaled, time_fit, time_pred),
      .fns = list(mean = mean,
                  median = median,
                  sd = sd)
    )
  ) %>%
  bind_rows(results_overall) %>%
  # mutate(
  #   data = if_else(
  #     is.na(n_z),
  #     true = data,
  #     false = as.character(
  #       glue::glue("Simulation, N junk = {n_z}, N obs = {n_obs}, X corr less than or equal to {correlated_x}")
  #     )
  #   )
  # ) %>%
  arrange(data, desc(ibs_scaled_mean)) %>%
  transmute(
    data = recode(data, !!!data_recoder),
    data = fct_relevel(factor(data), 'Overall'),
    model = recode(model, !!!model_recoder),
    ibs_scaled = table_glue(
      "{ibs_scaled_mean} ({ibs_scaled_sd})"
    ),
    cstat = table_glue(
      "{cstat_mean} ({cstat_sd})"
    ),
    time_fit_median = format(round(time_fit_median, 3), nsmall = 3),
    time_pred_median = format(round(time_pred_median, 3), nsmall = 3)
  ) %>%
  arrange(data)

indents <- table(data_tbl$data)

data_tbl %>%
  mutate(
    ibs_scaled = if_else(
      model == 'xgboost-aft',
      '---',
      ibs_scaled
    )
  ) %>%
  select(-data) %>%
  kable(booktabs=TRUE,
        longtable=TRUE,
        col.names = c(' ',
                      'Scaled Brier',
                      'C-Statistic',
                      'Model fitting',
                      'Risk prediction'),
        align = 'lcccc',
        caption = paste("Index of prediction accuracy, time-dependent concordance statistic, and computational time required to fit and compute predictions for several learning algorithms across", n_risk_tasks,  "risk prediction tasks.\\label{tab:bm_pred_all}")) %>%
  pack_rows(index = indents,
            italic = TRUE,
            hline_before = FALSE,
            hline_after = TRUE) %>%
  kable_styling(latex_options = c("repeat_header")) %>%
  add_header_above(header = c(" " = 1,
                              "Performance metric (SD)" = 2,
                              "Computation time, seconds" = 2))

@

<<>>=

bm_vi_smry <- bm_vi_viz$smry %>%
  select(-x, -data) %>%
  mutate(value = table_value(value*100)) %>%
  pivot_wider(values_from = value, names_from = model) %>%
  mutate(
    variable = factor(
      variable,
      levels = c("Overall",
                 "intr",
                 "nlin",
                 "cmbn",
                 "main"),
      labels = c("Overall",
                 "Interaction effects",
                 "Non-linear effects",
                 "Combination effects",
                 "Main effects")),
    pred_corr_max = if_else(
      is.na(pred_corr_max) | pred_corr_max == 1,
      "Overall",
      paste(as.integer(pred_corr_max * 100))
    ),
    n_obs = if_else(
      is.na(n_obs) | n_obs == 0,
      "Overall",
      table_value(as.integer(n_obs))
    )
  ) %>%
  select(
    "variable",
    "pred_corr_max",
    "n_obs",
    "aorsf-negate",
    "aorsf-anova",
    "aorsf-permute",
    "xgboost-shap",
    "xgboost-gain",
    "randomForestSRC-permutation"
  )

indents <- table(bm_vi_smry$variable)

bm_vi_smry %>%
  ungroup() %>%
  select(-variable) %>%
  kable(booktabs=TRUE,
        longtable=TRUE,
        align = paste("llcccccc"),
        col.names = c("Max correlation",
                      "No. observations",
                      "Negation",
                      "ANOVA",
                      "Permutation",
                      "SHAP",
                      "Gain",
                      "Permutation"),
        caption = "Discrimination of relevant versus irrelevant variables for several techniques to estimate variable importance. \\label{tab:bm_vi}") %>%
  kable_styling(latex_options = c("repeat_header")) %>%
  add_header_above(
    header = c(" " = 2,
               "accelerated oblique RSF" = 3,
               "xgboost" = 2,
               "RSF" = 1)
  ) %>%
  # collapse_rows(columns = 1) %>%
  pack_rows(group_label = 'Interactions',
            start_row = 2,
            end_row = 11,
            italic = TRUE,
            hline_before = FALSE,
            hline_after = TRUE) %>%
  pack_rows(group_label = 'Non-linear effects',
            start_row = 12,
            end_row = 21,
            italic = TRUE,
            hline_before = FALSE,
            hline_after = TRUE) %>%
pack_rows(group_label = 'Combination effects',
            start_row = 22,
            end_row = 31,
            italic = TRUE,
            hline_before = FALSE,
            hline_after = TRUE) %>%
  pack_rows(group_label = 'Main effects',
            start_row = 32,
            end_row = 41,
            italic = TRUE,
            hline_before = FALSE,
            hline_after = TRUE) %>%
  landscape()

@


\end{document}
