\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}
% extra packages for tables
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{hyperref}
\hypersetup{hidelinks}


\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Meil\u{a} and Jordan}
\firstpageno{1}

\begin{document}

\title{Accelerated oblique random survival forests}

\author{\name Marina Meil\u{a} \email mmp@stat.washington.edu \\
       \addr Department of Statistics\\
       University of Washington\\
       Seattle, WA 98195-4322, USA
       \AND
       \name Michael I.\ Jordan \email jordan@cs.berkeley.edu \\
       \addr Division of Computer Science and Department of Statistics\\
       University of California\\
       Berkeley, CA 94720-1776, USA}

\editor{Kevin Murphy and Bernhard Sch{\"o}lkopf}

\maketitle

<<echo = FALSE, include = FALSE>>=

setwd(here::here())
source("packages.R")
library(knitr)
library(kableExtra)

knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      dpi = 300)

rspec <- round_spec() |>
  round_using_magnitude(digits = c(3, 2, 1, 1),
                        breaks = c(1, 10, 100, Inf))

names(rspec) <- paste('table.glue', names(rspec), sep = '.')

options(rspec)

tar_load(names = c(bm_pred_comb, bm_vi_comb))

@


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
abstract here
\end{abstract}

\begin{keywords}
  Bayesian networks, mixture models, Chow-Liu trees
\end{keywords}

\section{Introduction}



\section{Related work}

Table \ref{table:1} is an example of a referenced \LaTeX{}. Several machine learning algorithms can engage with right-censored time-to-event outcomes. In the current study, we consider four classes of learners: random forests, boosting ensembles, regression models, and neural networks.

accelerated oblique random survival forests \texttt{aorsf}

original oblique random survival forests \texttt{obliqueRSF}

axis-based random survival forests \texttt{randomForestSRC \& ranger}

axis-based conditional inference forests \texttt{party}

gradient boosted decision trees \texttt{xgboost}


\begin{table}[h!]
\centering
\begin{tabular}{p{2cm} | p{3cm} p{7cm}}
 \hline
 Learner & Software & Description \\ [0.5ex]
 \hline\hline
 \multicolumn{3}{l}{\textit{Random Survival Forests}}\\
 \hline\hline
 Standard & \href{https://www.randomforestsrc.org/index.html}{\texttt{RandomForestSRC}} & Axis based survival trees following Leo Breiman's original random forest algorithm, with cut-points selected to maximize a log-rank statistic.  \\ \hline
 Oblique & \href{https://CRAN.R-project.org/package=obliqueRSF}{\texttt{obliqueRSF}} \newline \href{https://bcjaeger.github.io/aorsf/}{\texttt{aorsf}} & Oblique survival trees following Leo Breiman's random forest algorithm but not using random coefficients for linear combinations. \texttt{obliqueRSF} uses penalized models, and \texttt{aorsf} additionally uses partially trained models. Cut-points are selected to maximize a log-rank statistic. \\ \hline
 Extremely \,Randomized & \href{https://CRAN.R-project.org/package=ranger}{\texttt{ranger}} & Axis-based survival trees grown with randomly selected features and cut-points\\ \hline
 Conditional \,Inference & \href{http://party.r-forge.r-project.org/}{\texttt{party}} & Axis based survival trees grown using unbiased recursive partitioning.  \\
 \hline\hline
 \multicolumn{3}{l}{\textit{Boosting ensembles}}\\
 \hline\hline
 Trees & \href{https://xgboost.readthedocs.io/en/stable/#}{\texttt{xgboost}} & The Cox likelihood function is maximized additively with decision trees. Nested cross validation (5 folds) is applied to tune the number of trees grown.  \\
 Models & \href{https://xgboost.readthedocs.io/en/stable/#}{\texttt{xgboost}} & The accelerated failure time likelihood function is maximized additively with decision trees. Nested cross validation (5 folds) is applied to tune the number of trees grown.  \\
 \hline\hline
 \multicolumn{3}{l}{\textit{Regression models}}\\
 \hline\hline
 Cox Net & \texttt{glmnet} & The Cox model is fit using an elastic net penalty. Nested cross validation (5 folds) is applied to tune penalty terms.\\
 \hline\hline
 \multicolumn{3}{l}{\textit{Neural networks}}\\
 \hline\hline
 Cox Time & 6 & 87837  \\
 \hline
\end{tabular}
\caption{Table to test captions and labels.}
\label{table:1}
\end{table}

\section{Results}

<<>>=

results_overall <- bm_pred_comb |>
  group_by(model) |>
  summarize(
    across(
      .cols = c(cstat, IPA, time_fit, time_pred),
      .fns = list(mean = mean, sd = sd)
    )
  ) |>
  mutate(data = 'Overall')

data_tbl <- bm_pred_comb |>
  group_by(model, data, n_z, n_obs, correlated_x) |>
  summarize(
    across(
      .cols = c(cstat, IPA, time_fit, time_pred),
      .fns = list(mean = mean, sd = sd)
    )
  ) |>
  bind_rows(results_overall) |>
  mutate(
    data = if_else(
      is.na(n_z),
      true = data,
      false = as.character(
        glue::glue("Simulation, N junk = {n_z}, N obs = {n_obs}, X corr less then or equal to {correlated_x}")
      )
    )
  ) |>
  group_by(data) |>
  mutate(
    across(.cols = starts_with("time"), .fns = as.numeric),
    time_fit_ratio = time_fit_mean / time_fit_mean[model == 'aorsf_1'],
    time_pred_ratio = time_pred_mean / time_pred_mean[model == 'aorsf_1']
  ) |>
  arrange(data, desc(cstat_mean)) |>
  transmute(
    model = recode(
      model,
      "aorsf_15" = 'aORSF(i=15)',
      "aorsf_1" = 'aORSF(i=1)',
      "ranger" = 'Ranger',
      "xgboost" = 'Xgboost',
      "cif" = "Party",
      "randomForestSRC" = "Rfsrc",
      "coxtime" = "Coxtime"
    ),
    data = recode(
      data,
      'pbc_orsf' = 'Mayo Clinic Primary Biliary Cholangitis Data, N = 276',
      'rotterdam' = 'Rotterdam Breast Cancer Data, N = 2,982',
      'sim' = 'Simulated data'
    ),
    cstat = table_glue("{cstat_mean} ({cstat_sd})"),
    ipa = table_glue("{IPA_mean} ({IPA_sd})"),
    time_fit_mean = format(round(time_fit_mean, 3), nsmall = 3),
    time_fit_ratio = table_value(time_fit_ratio),
    time_pred_mean = format(round(time_pred_mean, 3), nsmall = 3),
    time_pred_ratio = table_value(time_pred_ratio)
  )

indents <- table(data_tbl$data)

data_tbl |>
  ungroup() |>
  select(-data) |>
  kable(booktabs=TRUE,
        longtable=TRUE,
        col.names = c(' ',
                      'C-Statistic',
                      'Scaled Brier',
                      'Mean',
                      'Ratio',
                      'Mean',
                      'Ratio'),
        align = 'lcc') |>
  pack_rows(index = indents,
            italic = TRUE,
            hline_before = TRUE,
            hline_after = TRUE) |>
  kable_styling(latex_options = c("repeat_header")) |>
  add_header_above(header = c(" " = 1,
                              "Performance metric (SD)" = 2,
                              "Fit model" = 2,
                              "Predict risk" = 2)) |>
  add_header_above(header = c(" " = 3,
                              "Computing time, seconds" = 4))


@


% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{main}

\end{document}
